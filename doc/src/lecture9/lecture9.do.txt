TITLE:  Many-body theory and nuclear few- and many-body systems
AUTHOR: Benjamin Hall and Morten Hjorth-Jensen {copyright, 2013-present|CC BY-NC} at "Facility for Rare Isotope Beams and National Superconducting Cyclotron Laboratory":"http://www.frib.msu.edu/" and "Department of Physics and Astronomy":"https://www.pa.msu.edu/", "Michigan State University":"http://www.msu.edu/", East Lansing, MI 48824, USA
DATE:  today



!split
===== Slater determinants as basis states =====
!bblock
The simplest possible choice for many-body wavefunctions are _product_ wavefunctions.
That is
!bt
\[ 
\Psi(x_1, x_2, x_3, \ldots, x_A) \approx \phi_1(x_1) \phi_2(x_2) \phi_3(x_3) \ldots
\]
!et
because we are really only good  at thinking about one particle at a time. Such 
product wavefunctions, without correlations, are easy to 
work with; for example, if the single-particle states $\phi_i(x)$ are orthonormal, then 
the product wavefunctions are easy to orthonormalize.   

Similarly, computing matrix elements of operators are relatively easy, because the 
integrals factorize.


The price we pay is the lack of correlations, which we must build up by using many, many product 
wavefunctions. (Thus we have a trade-off: compact representation of correlations but 
difficult integrals versus easy integrals but many states required.) 
!eblock

!split
===== Slater determinants as basis states, repetition =====
!bblock
Because we have fermions, we are required to have antisymmetric wavefunctions, e.g.
!bt
\[
\Psi(x_1, x_2, x_3, \ldots, x_A) = - \Psi(x_2, x_1, x_3, \ldots, x_A)
\]
!et
etc. This is accomplished formally by using the determinantal formalism
!bt
\[
\Psi(x_1, x_2, \ldots, x_A) 
= \frac{1}{\sqrt{A!}} 
\det \left | 
\begin{array}{cccc}
\phi_1(x_1) & \phi_1(x_2) & \ldots & \phi_1(x_A) \\
\phi_2(x_1) & \phi_2(x_2) & \ldots & \phi_2(x_A) \\
 \vdots & & &  \\
\phi_A(x_1) & \phi_A(x_2) & \ldots & \phi_A(x_A) 
\end{array}
\right |
\]
!et
Product wavefunction + antisymmetry = Slater determinant. 
!eblock

!split
===== Slater determinants as basis states =====
!bblock
!bt
\[
\Psi(x_1, x_2, \ldots, x_A) 
= \frac{1}{\sqrt{A!}} 
\det \left | 
\begin{array}{cccc}
\phi_1(x_1) & \phi_1(x_2) & \ldots & \phi_1(x_A) \\
\phi_2(x_1) & \phi_2(x_2) & \ldots & \phi_2(x_A) \\
 \vdots & & &  \\
\phi_A(x_1) & \phi_A(x_2) & \ldots & \phi_A(x_A) 
\end{array}
\right |
\]
!et
Properties of the determinant (interchange of any two rows or 
any two columns yields a change in sign; thus no two rows and no 
two columns can be the same) lead to the Pauli principle:

* No two particles can be at the same place (two columns the same); and
* No two particles can be in the same state (two rows the same).

!eblock


!split
===== Slater determinants as basis states =====
!bblock
As a practical matter, however, Slater determinants beyond $N=4$ quickly become 
unwieldy. Thus we turn to the _occupation representation_ or _second quantization_ to simplify calculations. 

The occupation representation or number representation, using fermion _creation_ and _annihilation_ 
operators, is compact and efficient. It is also abstract and, at first encounter, not easy to 
internalize. It is inspired by other operator formalism, such as the ladder operators for 
the harmonic oscillator or for angular momentum, but unlike those cases, the operators _do not have coordinate space representations_.

Instead, one can think of fermion creation/annihilation operators as a game of symbols that 
compactly reproduces what one would do, albeit clumsily, with full coordinate-space Slater 
determinants. 
!eblock

!split
===== Reminder  on the occupation representation (second quantization) =====
!bblock
We start with a set of orthonormal single-particle states $\{ \phi_i(x) \}$. 
(Note: this requirement, and others, can be relaxed, but leads to a 
more involved formalism.) _Any_ orthonormal set will do. 

To each single-particle state $\phi_i(x)$ we associate a creation operator 
$\hat{a}^\dagger_i$ and an annihilation operator $\hat{a}_i$. 

When acting on the vacuum state $| 0 \rangle$, the creation operator $\hat{a}^\dagger_i$ causes 
a particle to occupy the single-particle state $\phi_i(x)$:
!bt
\[
\phi_i(x) \rightarrow \hat{a}^\dagger_i |0 \rangle
\]
!et
!eblock

!split
===== Quick repetition  of the occupation representation =====
!bblock
But with multiple creation operators we can occupy multiple states:
!bt
\[
\phi_i(x) \phi_j(x^\prime) \phi_k(x^{\prime \prime}) 
\rightarrow \hat{a}^\dagger_i \hat{a}^\dagger_j \hat{a}^\dagger_k |0 \rangle.
\]
!et

Now we impose antisymmetry, by having the fermion operators satisfy  _anticommutation relations_:
!bt
\[
\hat{a}^\dagger_i \hat{a}^\dagger_j + \hat{a}^\dagger_j \hat{a}^\dagger_i
= [ \hat{a}^\dagger_i ,\hat{a}^\dagger_j ]_+ 
= \{ \hat{a}^\dagger_i ,\hat{a}^\dagger_j \} = 0
\]
!et
so that 
!bt
\[
\hat{a}^\dagger_i \hat{a}^\dagger_j = - \hat{a}^\dagger_j \hat{a}^\dagger_i
\]
!et
!eblock


!split
===== Quick repetition  of the occupation representation =====
!bblock
Because of this property, automatically $\hat{a}^\dagger_i \hat{a}^\dagger_i = 0$, 
enforcing the Pauli exclusion principle.  Thus when writing a Slater determinant 
using creation operators, 
!bt
\[
\hat{a}^\dagger_i \hat{a}^\dagger_j \hat{a}^\dagger_k \ldots |0 \rangle
\]
!et
each index $i,j,k, \ldots$ must be unique.

For some relevant exercises with solutions see chapter 8 of "Lecture Notes in Physics, volume 936":"http://www.springer.com/us/book/9783319533353".

!eblock


!split
===== Full Configuration Interaction Theory =====
!bblock
We have defined the ansatz for the ground state as 
!bt
\[
|\Phi_0\rangle = \left(\prod_{i\le F}\hat{a}_{i}^{\dagger}\right)|0\rangle,
\]
!et
where the index $i$ defines different single-particle states up to the Fermi level. We have assumed that we have $N$ fermions. 
A given one-particle-one-hole ($1p1h$) state can be written as
!bt
\[
|\Phi_i^a\rangle = \hat{a}_{a}^{\dagger}\hat{a}_i|\Phi_0\rangle,
\]
!et
while a $2p2h$ state can be written as
!bt
\[
|\Phi_{ij}^{ab}\rangle = \hat{a}_{a}^{\dagger}\hat{a}_{b}^{\dagger}\hat{a}_j\hat{a}_i|\Phi_0\rangle,
\]
!et
and a general $NpNh$ state as 
!bt
\[
|\Phi_{ijk\dots}^{abc\dots}\rangle = \hat{a}_{a}^{\dagger}\hat{a}_{b}^{\dagger}\hat{a}_{c}^{\dagger}\dots\hat{a}_k\hat{a}_j\hat{a}_i|\Phi_0\rangle.
\]
!et
!eblock

!split
===== Full Configuration Interaction Theory =====
!bblock
We can then expand our exact state function for the ground state 
as
!bt
\[
|\Psi_0\rangle=C_0|\Phi_0\rangle+\sum_{ai}C_i^a|\Phi_i^a\rangle+\sum_{abij}C_{ij}^{ab}|\Phi_{ij}^{ab}\rangle+\dots
=(C_0+\hat{C})|\Phi_0\rangle,
\]
!et
where we have introduced the so-called correlation operator 
!bt
\[
\hat{C}=\sum_{ai}C_i^a\hat{a}_{a}^{\dagger}\hat{a}_i  +\sum_{abij}C_{ij}^{ab}\hat{a}_{a}^{\dagger}\hat{a}_{b}^{\dagger}\hat{a}_j\hat{a}_i+\dots
\]
!et
Since the normalization of $\Psi_0$ is at our disposal and since $C_0$ is by hypothesis non-zero, we may arbitrarily set $C_0=1$ with 
corresponding proportional changes in all other coefficients. Using this so-called intermediate normalization we have
!bt
\[
\langle \Psi_0 | \Phi_0 \rangle = \langle \Phi_0 | \Phi_0 \rangle = 1, 
\]
!et
resulting in 
!bt
\[
|\Psi_0\rangle=(1+\hat{C})|\Phi_0\rangle.
\]
!et
!eblock

!split
===== Full Configuration Interaction Theory =====
!bblock
We rewrite 
!bt
\[
|\Psi_0\rangle=C_0|\Phi_0\rangle+\sum_{ai}C_i^a|\Phi_i^a\rangle+\sum_{abij}C_{ij}^{ab}|\Phi_{ij}^{ab}\rangle+\dots,
\]
!et
in a more compact form as 
!bt
\[
|\Psi_0\rangle=\sum_{PH}C_H^P\Phi_H^P=\left(\sum_{PH}C_H^P\hat{A}_H^P\right)|\Phi_0\rangle,
\]
!et
where $H$ stands for $0,1,\dots,n$ hole states and $P$ for $0,1,\dots,n$ particle states. 
Our requirement of unit normalization gives
!bt
\[
\langle \Psi_0 | \Phi_0 \rangle = \sum_{PH}|C_H^P|^2= 1,
\]
!et
and the energy can be written as 
!bt
\[
E= \langle \Psi_0 | \hat{H} |\Phi_0 \rangle= \sum_{PP'HH'}C_H^{*P}\langle \Phi_H^P | \hat{H} |\Phi_{H'}^{P'} \rangle C_{H'}^{P'}.
\]
!et
!eblock

!split
===== Full Configuration Interaction Theory =====
!bblock
Normally 
!bt
\[
E= \langle \Psi_0 | \hat{H} |\Phi_0 \rangle= \sum_{PP'HH'}C_H^{*P}\langle \Phi_H^P | \hat{H} |\Phi_{H'}^{P'} \rangle C_{H'}^{P'},
\]
!et
is solved by diagonalization setting up the Hamiltonian matrix defined by the basis of all possible Slater determinants. A diagonalization
# to do: add text about Rayleigh-Ritz
is equivalent to finding the variational minimum   of 
!bt
\[
 \langle \Psi_0 | \hat{H} |\Phi_0 \rangle-\lambda \langle \Psi_0 |\Phi_0 \rangle,
\]
!et
where $\lambda$ is a variational multiplier to be identified with the energy of the system.
The minimization process results in 
!bt
\[
\delta\left[ \langle \Psi_0 | \hat{H} |\Phi_0 \rangle-\lambda \langle \Psi_0 |\Phi_0 \rangle\right]=
\]
!et
!bt
\[
\sum_{P'H'}\left\{\delta[C_H^{*P}]\langle \Phi_H^P | \hat{H} |\Phi_{H'}^{P'} \rangle C_{H'}^{P'}+
C_H^{*P}\langle \Phi_H^P | \hat{H} |\Phi_{H'}^{P'} \rangle \delta[C_{H'}^{P'}]-
\lambda( \delta[C_H^{*P}]C_{H'}^{P'}+C_H^{*P}\delta[C_{H'}^{P'}]\right\} = 0.
\]
!et
Since the coefficients $\delta[C_H^{*P}]$ and $\delta[C_{H'}^{P'}]$ are complex conjugates it is necessary and sufficient to require the quantities that multiply with $\delta[C_H^{*P}]$ to vanish.  
!eblock



!split
===== Full Configuration Interaction Theory =====
!bblock

This leads to 
!bt
\[
\sum_{P'H'}\langle \Phi_H^P | \hat{H} |\Phi_{H'}^{P'} \rangle C_{H'}^{P'}-\lambda C_H^{P}=0,
\]
!et
for all sets of $P$ and $H$.

If we then multiply by the corresponding $C_H^{*P}$ and sum over $PH$ we obtain
!bt
\[ 
\sum_{PP'HH'}C_H^{*P}\langle \Phi_H^P | \hat{H} |\Phi_{H'}^{P'} \rangle C_{H'}^{P'}-\lambda\sum_{PH}|C_H^P|^2=0,
\]
!et
leading to the identification $\lambda = E$. This means that we have for all $PH$ sets
!bt
\begin{equation}
\sum_{P'H'}\langle \Phi_H^P | \hat{H} -E|\Phi_{H'}^{P'} \rangle = 0. label{eq:fullci}
\end{equation}
!et
!eblock



!split
===== Full Configuration Interaction Theory =====
!bblock
An alternative way to derive the last equation is to start from 
!bt
\[
(\hat{H} -E)|\Psi_0\rangle = (\hat{H} -E)\sum_{P'H'}C_{H'}^{P'}|\Phi_{H'}^{P'} \rangle=0, 
\]
!et
and if this equation is successively projected against all $\Phi_H^P$ in the expansion of $\Psi$, then the last equation on the previous slide
results.   As stated previously, one solves this equation normally by diagonalization. If we are able to solve this equation exactly (that is
numerically exactly) in a large Hilbert space (it will be truncated in terms of the number of single-particle states included in the definition
of Slater determinants), it can then serve as a benchmark for other many-body methods which approximate the correlation operator
$\hat{C}$.  
!eblock



!split
=====  Shell-model jargon =====
!bblock
If we do not make any truncations in the possible sets of Slater determinants (many-body states) we can make by distributing $A$ nucleons among $n$ single-particle states, we call such a calculation for _Full configuration interaction theory_

If we make truncations, we have different possibilities

* The standard nuclear shell-model. Here we define an effective Hilbert space with respect to a given core. The calculations are normally then performed for all many-body states that can be constructed from the effective Hilbert spaces. This approach requires a properly defined effective Hamiltonian
* We can truncate in the number of excitations. For example, we can limit the possible Slater determinants to only $1p-1h$ and $2p-2h$ excitations. This is called a configuration interaction calculation at the level of singles and doubles excitations, or just CISD. 
* We can limit the number of excitations in terms of the excitation energies. If we do not define a core, this defines normally what is called the no-core shell-model approach. 

What happens if we have a three-body interaction and a Hartree-Fock basis? 
!eblock

!split
=====  FCI and the exponential growth =====
!bblock
Full configuration interaction theory calculations provide in principle, if we can diagonalize numerically, all states of interest. The dimensionality of the problem explodes however quickly.

The total number of Slater determinants which can be built with say $N$ neutrons distributed among $n$ single particle states is
!bt
\[
\left (\begin{array}{c} n \\ N\end{array} \right) =\frac{n!}{(n-N)!N!}. 
\]
!et

For a model space which comprises the first for major shells only $0s$, $0p$, $1s0d$ and $1p0f$ we have $40$ single particle states for neutrons and protons.  For the eight neutrons of oxygen-16 we would then have
!bt
\[
\left (\begin{array}{c} 40 \\ 8\end{array} \right) =\frac{40!}{(32)!8!}\sim 10^{9}, 
\]
!et
and multiplying this with the number of proton Slater determinants we end up with approximately with a dimensionality $d$ of $d\sim 10^{18}$.
!eblock


!split
=====  Exponential wall =====
!bblock
This number can be reduced if we look at specific symmetries only. However, the dimensionality explodes quickly!

* For Hamiltonian matrices of dimensionalities  which are smaller than $d\sim 10^5$, we would use so-called direct methods for diagonalizing the Hamiltonian matrix
* For larger dimensionalities iterative eigenvalue solvers like Lanczos' method are used. The most efficient codes at present can handle matrices of $d\sim 10^{10}$. 
!eblock


!split 
===== A non-practical way of solving the eigenvalue problem =====
!bblock
To see this, we look at the contributions arising from 
!bt
\[
\langle \Phi_H^P | = \langle \Phi_0|
\]
!et
in  Eq.~(ref{eq:fullci}), that is we multiply with $\langle \Phi_0 |$
from the left in 
!bt
\[
(\hat{H} -E)\sum_{P'H'}C_{H'}^{P'}|\Phi_{H'}^{P'} \rangle=0. 
\]
!et
If we assume that we have a two-body operator at most, Slater's rule gives then an equation for the 
correlation energy in terms of $C_i^a$ and $C_{ij}^{ab}$ only.  We get then
!bt
\[
\langle \Phi_0 | \hat{H} -E| \Phi_0\rangle + \sum_{ai}\langle \Phi_0 | \hat{H} -E|\Phi_{i}^{a} \rangle C_{i}^{a}+
\sum_{abij}\langle \Phi_0 | \hat{H} -E|\Phi_{ij}^{ab} \rangle C_{ij}^{ab}=0,
\]
!et
or 
!bt
\[
E-E_0 =\Delta E=\sum_{ai}\langle \Phi_0 | \hat{H}|\Phi_{i}^{a} \rangle C_{i}^{a}+
\sum_{abij}\langle \Phi_0 | \hat{H}|\Phi_{ij}^{ab} \rangle C_{ij}^{ab},
\]
!et
where the energy $E_0$ is the reference energy and $\Delta E$ defines the so-called correlation energy.
The single-particle basis functions  could be the results of a Hartree-Fock calculation or just the eigenstates of the non-interacting part of the Hamiltonian. 
!eblock


!split 
===== A non-practical way of solving the eigenvalue problem =====
!bblock
To see this, we look at the contributions arising from 
!bt
\[
\langle \Phi_H^P | = \langle \Phi_0|
\]
!et
in  Eq.~(ref{eq:fullci}), that is we multiply with $\langle \Phi_0 |$
from the left in 
!bt
\[
(\hat{H} -E)\sum_{P'H'}C_{H'}^{P'}|\Phi_{H'}^{P'} \rangle=0. 
\]
!et
!eblock


!split 
===== A non-practical way of solving the eigenvalue problem =====
!bblock
If we assume that we have a two-body operator at most, Slater's rule gives then an equation for the 
correlation energy in terms of $C_i^a$ and $C_{ij}^{ab}$ only.  We get then
!bt
\[
\langle \Phi_0 | \hat{H} -E| \Phi_0\rangle + \sum_{ai}\langle \Phi_0 | \hat{H} -E|\Phi_{i}^{a} \rangle C_{i}^{a}+
\sum_{abij}\langle \Phi_0 | \hat{H} -E|\Phi_{ij}^{ab} \rangle C_{ij}^{ab}=0,
\]
!et
or 
!bt
\[
E-E_0 =\Delta E=\sum_{ai}\langle \Phi_0 | \hat{H}|\Phi_{i}^{a} \rangle C_{i}^{a}+
\sum_{abij}\langle \Phi_0 | \hat{H}|\Phi_{ij}^{ab} \rangle C_{ij}^{ab},
\]
!et
where the energy $E_0$ is the reference energy and $\Delta E$ defines the so-called correlation energy.
The single-particle basis functions  could be the results of a Hartree-Fock calculation or just the eigenstates of the non-interacting part of the Hamiltonian. 
!eblock



!split
=====  Rewriting the FCI equation  =====
!bblock
In our notes on Hartree-Fock calculations, 
we have already computed the matrix $\langle \Phi_0 | \hat{H}|\Phi_{i}^{a}\rangle $ and $\langle \Phi_0 | \hat{H}|\Phi_{ij}^{ab}\rangle$.  If we are using a Hartree-Fock basis, then the matrix elements
$\langle \Phi_0 | \hat{H}|\Phi_{i}^{a}\rangle=0$ and we are left with a *correlation energy* given by
!bt
\[
E-E_0 =\Delta E^{HF}=\sum_{abij}\langle \Phi_0 | \hat{H}|\Phi_{ij}^{ab} \rangle C_{ij}^{ab}. 
\]
!et
!eblock
 
!split
=====  Rewriting the FCI equation  =====
!bblock
Inserting the various matrix elements we can rewrite the previous equation as
!bt
\[
\Delta E=\sum_{ai}\langle i| \hat{f}|a \rangle C_{i}^{a}+
\sum_{abij}\langle ij | \hat{v}| ab \rangle C_{ij}^{ab}.
\]
!et
This equation determines the correlation energy but not the coefficients $C$. 
!eblock

!split
=====  Rewriting the FCI equation, does not stop here  =====
!bblock
We need more equations. Our next step is to set up
!bt
\[
\langle \Phi_i^a | \hat{H} -E| \Phi_0\rangle + \sum_{bj}\langle \Phi_i^a | \hat{H} -E|\Phi_{j}^{b} \rangle C_{j}^{b}+
\sum_{bcjk}\langle \Phi_i^a | \hat{H} -E|\Phi_{jk}^{bc} \rangle C_{jk}^{bc}+
\sum_{bcdjkl}\langle \Phi_i^a | \hat{H} -E|\Phi_{jkl}^{bcd} \rangle C_{jkl}^{bcd}=0,
\]
!et
as this equation will allow us to find an expression for the coefficents $C_i^a$ since we can rewrite this equation as 
!bt
\[
\langle i | \hat{f}| a\rangle +\langle \Phi_i^a | \hat{H}|\Phi_{i}^{a} \rangle C_{i}^{a}+ \sum_{bj\ne ai}\langle \Phi_i^a | \hat{H}|\Phi_{j}^{b} \rangle C_{j}^{b}+
\sum_{bcjk}\langle \Phi_i^a | \hat{H}|\Phi_{jk}^{bc} \rangle C_{jk}^{bc}+
\sum_{bcdjkl}\langle \Phi_i^a | \hat{H}|\Phi_{jkl}^{bcd} \rangle C_{jkl}^{bcd}=EC_i^a.
\]
!et
!eblock

!split
=====  Rewriting the FCI equation, please stop here  =====
!bblock
We see that on the right-hand side we have the energy $E$. This leads to a non-linear equation in the unknown coefficients. 
These equations are normally solved iteratively ( that is we can start with a guess for the coefficients $C_i^a$). A common choice is to use perturbation theory for the first guess, setting thereby
!bt
\[
 C_{i}^{a}=\frac{\langle i | \hat{f}| a\rangle}{\epsilon_i-\epsilon_a}.
\]
!et
!eblock

!split
=====  Rewriting the FCI equation, more to add  =====
!bblock
The observant reader will however see that we need an equation for $C_{jk}^{bc}$ and $C_{jkl}^{bcd}$ as well.
To find equations for these coefficients we need then to continue our multiplications from the left with the various
$\Phi_{H}^P$ terms. 


For $C_{jk}^{bc}$ we need then
!bt
\[
\langle \Phi_{ij}^{ab} | \hat{H} -E| \Phi_0\rangle + \sum_{kc}\langle \Phi_{ij}^{ab} | \hat{H} -E|\Phi_{k}^{c} \rangle C_{k}^{c}+
\]
!et
!bt
\[
\sum_{cdkl}\langle \Phi_{ij}^{ab} | \hat{H} -E|\Phi_{kl}^{cd} \rangle C_{kl}^{cd}+\sum_{cdeklm}\langle \Phi_{ij}^{ab} | \hat{H} -E|\Phi_{klm}^{cde} \rangle C_{klm}^{cde}+\sum_{cdefklmn}\langle \Phi_{ij}^{ab} | \hat{H} -E|\Phi_{klmn}^{cdef} \rangle C_{klmn}^{cdef}=0,
\]
!et
and we can isolate the coefficients $C_{kl}^{cd}$ in a similar way as we did for the coefficients $C_{i}^{a}$. 
!eblock



!split
=====  Rewriting the FCI equation, more to add  =====
!bblock
A standard choice for the first iteration is to set 
!bt
\[
C_{ij}^{ab} =\frac{\langle ij \vert \hat{v} \vert ab \rangle}{\epsilon_i+\epsilon_j-\epsilon_a-\epsilon_b}.
\]
!et
At the end we can rewrite our solution of the Schroedinger equation in terms of $n$ coupled equations for the coefficients $C_H^P$.
This is a very cumbersome way of solving the equation. However, by using this iterative scheme we can illustrate how we can compute the
various terms in the wave operator or correlation operator $\hat{C}$. We will later identify the calculation of the various terms $C_H^P$
as parts of different many-body approximations to full CI. In particular, we can  relate this non-linear scheme with Coupled Cluster theory and
many-body perturbation theory.
!eblock


!split
===== Summarizing FCI and bringing in approximative methods =====
!bblock

If we can diagonalize large matrices, FCI is the method of choice since:
* It gives all eigenvalues, ground state and excited states
* The eigenvectors are obtained directly from the coefficients $C_H^P$ which result from the diagonalization
* We can compute easily expectation values of other operators, as well as transition probabilities
* Correlations are easy to understand in terms of contributions to a given operator beyond the Hartree-Fock contribution. This is the standard approach in  many-body theory. 
!eblock

!split
=====  Definition of the correlation energy  =====
!bblock
The correlation energy is defined as, with a two-body Hamiltonian,  
!bt
\[
\Delta E=\sum_{ai}\langle i| \hat{f}|a \rangle C_{i}^{a}+
\sum_{abij}\langle ij | \hat{v}| ab \rangle C_{ij}^{ab}.
\]
!et
The coefficients $C$ result from the solution of the eigenvalue problem. 
The energy of say the ground state is then
!bt
\[
E=E_{ref}+\Delta E,
\]
!et
where the so-called reference energy is the energy we obtain from a Hartree-Fock calculation, that is
!bt
\[
E_{ref}=\langle \Phi_0 \vert \hat{H} \vert \Phi_0 \rangle.
\]
!et

!eblock

!split
===== FCI equation and the coefficients  =====
!bblock

However, as we have seen, even for a small case like the four first major shells and a nucleus like oxygen-16, the dimensionality becomes quickly intractable. If we wish to include single-particle states that reflect weakly bound systems, we need a much larger single-particle basis. We need thus approximative methods that sum specific correlations to infinite order. 

Popular methods are
* "Many-body perturbation theory (in essence a Taylor expansion)":"http://www.sciencedirect.com/science/article/pii/0370157395000126"
* "Coupled cluster theory (coupled non-linear equations)":"http://iopscience.iop.org/article/10.1088/0034-4885/77/9/096302/meta"
* Green's function approaches (matrix inversion)
* "Similarity group transformation methods (coupled ordinary differential equations)":"http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.106.222502"

All these methods start normally with a Hartree-Fock basis as the calculational basis. 
!eblock

!split
=====  Coupled Cluster theory =====

Add material here


!split
===== Example case: pairing Hamiltonian =====
!bblock

Now we take the following Hamiltonian
!bt
\[
\hat{H} = \sum_n n \delta \hat{N}_n  - G \hat{P}^\dagger \hat{P}
\]
!et
where 
!bt
\[
\hat{N}_n = \hat{a}^\dagger_{n, m=+1/2} \hat{a}_{n, m=+1/2} +
\hat{a}^\dagger_{n, m=-1/2} \hat{a}_{n, m=-1/2}
\]
!et
and
!bt 
\[
\hat{P}^\dagger = \sum_{n} \hat{a}^\dagger_{n, m=+1/2} \hat{a}^\dagger_{n, m=-1/2} 
\]
!et
We can write down the $ 6 \times 6$  Hamiltonian in the basis from the prior slide:
!bt
\[
H = \left ( 
\begin{array}{cccccc}
2\delta -2G & -G & -G & -G & -G & 0 \\
 -G & 4\delta -2G & -G & -G & -0 & -G \\
-G & -G & 6\delta -2G & 0 & -G & -G \\
 -G & -G & 0 & 6\delta-2G & -G & -G \\
 -G & 0 & -G & -G & 8\delta-2G & -G \\
0 & -G & -G & -G & -G & 10\delta -2G 
\end{array} \right )
\]
!et
(You should check by hand that this is correct.) 

For $\delta = 0$ we have the closed form solution of  the g.s. energy given by $-6G$.

!eblock










The coupled-cluster method is an efficient tool to compute atomic
nuclei with an effort that grows polynomial with system size. While
this might still be expensive, it is now possible to compute nuclei
with mass numbers about $A\approx 100$ with this method. Recall that
full configuration interaction (FCI) such as the no-core shell model
exhibits an exponential cost and is therefore limited to light nuclei.



======= The normal-ordered Hamiltonian =======

We start from the reference state

!bt
\begin{equation}
\label{HFref}
\vert\Phi_0\rangle = \prod_{i=1}^A a^\dagger_i \vert 0\rangle 
\end{equation}
!et

for the description of a nucleus with mass number $A$.  Usually, this
reference is the Hartree-Fock state, but that is not necessary. In the
shell-model picture, it could also be a product state where the lowest
$A$ harmonic oscillator states are occupied.  Here and in what
follows, the indices $i,j,k,\ldots$ run over hole states,
i.e. orbitals occupied in the reference state (ref{HFref}), while
$a,b,c,\ldots$ run over particle states, i.e. unoccupied
orbitals. Indices $p,q,r,s$ can identify any orbital.  Let $n_u$ be
the number of unoccupied states, and $A$ is of course the number of
occupied states. We consider the Hamiltonian

!bt
\begin{equation}
\label{Ham} H =
\sum_{pq} \varepsilon^p_q a^\dagger_p a_q +
\frac{1}{4}\sum_{pqrs}\langle pq\vert V\vert rs\rangle
a^\dagger_pa^\dagger_q a_sa_r
\end{equation}
!et



The reference state (ref{HFref}) is a non-trivial vacuum of our theory. 
We normal order this Hamiltonian with respect to the nontrivial vacuum
state given by the Hartree-Fock reference and obtain the
normal-ordered Hamiltonian

!bt
\begin{equation}
\label{HN}
H_N = \sum_{pq} f_{pq} \left\{a^\dagger_p a_q\right\} + \frac{1}{4}\sum_{pqrs}\langle pq\vert V\vert rs\rangle \left\{a^\dagger_pa^\dagger_q a_sa_r\right\}.
\end{equation}
!et

Here,

!bt
\begin{equation}
\label{Fock}
f^p_q = \varepsilon^p_q + \sum_i \langle pi\vert V\vert qi\rangle
\end{equation}
!et

is the Fock matrix. We note that the Fock matrix is diagonal in the
Hartree-Fock basis. The brackets $\{\cdots\}$ in Eq. (ref{HN}) denote
normal ordering, i.e. all operators that annihilate the nontrivial
vaccum (ref{HFref}) are to the right of those operators that create
with respect to that vaccum. Normal ordering implies that $\langle
\Phi_0\vert H_N\vert \Phi_0\rangle = 0$.


!bblock
===== Exercise: Practice in normal ordering =====
label{ex-1}

Normal order the expression $\sum\limits_{pq}\varepsilon_q^p a^\dagger_p a_q$.

!bhint
!bt
\begin{align}
\sum_{pq}\varepsilon_q^p a^\dagger_p a_q
=\sum_{ab}\varepsilon_b^a a^\dagger_a a_b
+\sum_{ai}\varepsilon_i^a a^\dagger_a a_i
+\sum_{ai}\varepsilon_a^i a^\dagger_i a_a   
+\sum_{ij}\varepsilon_j^i a^\dagger_i a_j
\end{align}
!et
!ehint

!bans
We have to move all operators that annihilate the reference state to the right of those that create on the reference state. Thus,
!bt
\begin{align}
\sum_{pq}\varepsilon_q^p a^\dagger_p a_q
&=\sum_{ab}\varepsilon_b^a a^\dagger_a a_b
+\sum_{ai}\varepsilon_i^a a^\dagger_a a_i
+\sum_{ai}\varepsilon_a^i a^\dagger_i a_a
+\sum_{ij}\varepsilon_j^i a^\dagger_i a_j\\
&=\sum_{ab}\varepsilon_b^a a^\dagger_a a_b
+\sum_{ai}\varepsilon_i^a a^\dagger_a a_i
+\sum_{ai}\varepsilon_a^i a^\dagger_i a_a
+\sum_{ij}\varepsilon_j^i \left(-a_ja^\dagger_i +\delta_i^j\right)\\
&=\sum_{ab}\varepsilon_b^a a^\dagger_a a_b
+\sum_{ai}\varepsilon_i^a a^\dagger_a a_i
+\sum_{ai}\varepsilon_a^i a^\dagger_i a_a
-\sum_{ij}\varepsilon_j^i a_ja^\dagger_i +\sum_i \varepsilon_i^i\\
&=\sum_{pq}\varepsilon_q^p \left\{a^\dagger_p a_q\right\} +\sum_i \varepsilon_i^i
\end{align}
!et
!eans
===== =====
!eblock


We note that $H = E_{HF} + H_N$, where

!bt
\begin{align}
E_{HF} &\equiv \langle\Phi_0\vert H\vert \Phi_0\rangle = \sum_{i} \varepsilon^i_i +\frac{1}{2}\sum_{ij}\langle ij\vert V\vert ij\rangle
\end{align}
!et

is the Hartree-Fock energy.
The coupled-cluster method is a very efficient tool to compute nuclei
when a ``good'' reference state is available. Let us assume that the
reference state results from a Hartree-Fock calculation.






!bblock
===== Exercise: What does ``good'' mean? =====
label{ex-2}

How do you know whether a Hartree-Fock state is a ``good'' reference?
Which results of the Hartree-Fock computation will inform you?

!bans

Once the Hartree-Fock equations are solved, the Fock matrix
(ref{Fock}) becomes diagonal, and its diagonal elements can be viewed
as single-particle energies. Hopefully, there is a clear gap in the
single-particle spectrum at the Fermi surface, i.e. after $A$ orbitals
are filled.

!eans
===== =====
!eblock

If symmetry-restricted Hartree-Fock is used, one is limited to compute
nuclei with closed subshells for neutrons and for protons. On a first
view, this might seem as a severe limitation. But is it? 


!bblock

===== Exercise: How many nuclei are accessible with the coupled cluster method based on spherical mean fields? =====
label{ex-3}
If one limits oneself to nuclei with mass number up to
mass number $A=60$, how many nuclei can potentially be described with
the coupled-cluster method? Which of these nuclei are potentially
interesting? Why?

!bans

Nuclear shell closures are at $N,Z=2,8,20,28,50,82,126$, and subshell
closures at $N,Z=2,6,8,14,16,20,28,32,34,40,50,\ldots$. 

In the physics of nuclei, the evolution of nuclear structure as
neutrons are added (or removed) from an isotope is a key
#interest. Examples are the rare isotopes of helium ($^{8,10}$ He)
#oxygen ($^{22,24,28}$ O), calcium ($^{52,54,60}$ Ca), nickel ($^{78}$
#Ni) and tin ($^{100,132}$ Sn). The coupled-cluster method has the
interest. Examples are the rare isotopes of helium (He-8,10)
oxygen (O-22,24,28), calcium (Ca-52,54,60), nickel (Ni-78) and tin
(Sn-100,132). The coupled-cluster method has the
potential to address questions regarding these nuclei, and in a
several cases was used to make predictions before experimental data
was available. In addition, the method can be used to compute
neighbors of nuclei with closed subshells.

!eans
===== =====
!eblock


======= The similarity transformed Hamiltonian =======


There are several ways to view and understand the coupled-cluster
method. A first simple view of coupled-cluster theory is that the
method induces correlations into the reference state by expressing a
correlated state as

!bt
\begin{equation}
\label{psi}
\vert\Psi\rangle = e^T \vert\Phi_0\rangle ,
\end{equation}
!et

Here, $T$ is an operator that induces correlations. We can now demand
that the correlate state (ref{psi}) becomes and eigenstate of the
Hamiltonian $H_N$, i.e.  $H_N\vert \Psi\rangle = E\vert \Psi\rangle$. This view,
while correct, is not the most productive one.  Instead, we
left-multiply the Schroedinger equation with $e^{-T}$ and find

!bt
\begin{equation}
\label{Schroedinger}
\overline{H_N}\vert \Phi_0\rangle = E_c \vert \Phi_0\rangle . 
\end{equation}
!et

Here, $E_c$ is the correlation energy, and the total energy is
$E=E_c+E_{HF}$.  The similarity-transformed Hamiltonian is defined as

!bt
\begin{equation}
\label{Hsim}
\overline{H_N} \equiv e^{-T} H_N e^T .
\end{equation}
!et


A more productive view on coupled-cluster theory thus emerges: This
method seeks a similarity transformation such that the uncorrelated
reference state (ref{HFref}) becomes an exact eigenstate of the
similarity-transformed Hamiltonian (ref{Hsim}).

!bblock
===== Exercise: What $T$ leads to Hermitian $\overline{H_N}$ ? =====
label{ex-4}
What are the conditions on $T$ such that $\overline{H_N}$ is Hermitian?

!bans

For a Hermitian $\overline{H_N}$, we need a unitary $e^T$, i.e. an
anti-Hermitian $T$ with $T = -T^\dagger$

!eans
===== =====
!eblock

As we will see below, coupld-cluster theory employs a non-Hermitian Hamiltonian.

!bblock
===== Exercise: Understanding (non-unitary) similarity transformations =====
label{ex-5}
Show that $\overline{H_N}$ has the same eigenvalues as $H_N$ for
arbitrary $T$. What is the spectral decomposition of a non-Hermitian
$\overline{H_N}$ ?

!bans

Let $H_N\vert E\rangle = E\vert E\rangle$. Thus

!bt
\begin{align*}
H_N e^{T} e^{-T} \vert E\rangle &= E\vert E\rangle , \\
\left(e^{-T} H_N e^T\right) e^{-T} \vert E\rangle &= Ee^{-T} \vert E\rangle , \\
\overline{H_N} e^{-T} \vert E\rangle &= E e^{-T}\vert E\rangle .
\end{align*}
!et

Thus, if $\vert E\rangle$ is an eigenstate of $H_N$ with eigenvalue $E$,
then $e^{-T}\vert E\rangle$ is eigenstate of $\overline{H_N}$ with the same
eigenvalue.

A non-Hermitian $\overline{H_N}$ has eigenvalues $E_\alpha$
corresponding to left $\langle L_\alpha\vert $ and right $\vert R_\alpha
\rangle$ eigenstates. Thus

!bt
\begin{align}
\overline{H_N} = \sum_\alpha \vert R_\alpha\rangle E_\alpha \langle L_\alpha \vert 
\end{align}
!et
with bi-orthonormal $\langle L_\alpha\vert R_\beta\rangle = \delta_\alpha^\beta$. 

!eans
===== =====
!eblock


To make progress, we have to specify the cluster operator $T$. In
coupled cluster theory, this operator is

!bt
\begin{equation}
\label{Top}
T \equiv \sum_{ia} t_i^a a^\dagger_a a_i + \frac{1}{4}\sum_{ijab}t_{ij}^{ab}
a^\dagger_aa^\dagger_ba_ja_i + \cdots
+ \frac{1}{(A!)^2}\sum_{i_1\ldots i_A a_1 \ldots a_A}
t_{i_1\ldots i_A}^{a_1\ldots a_A} a^\dagger_{a_1}\cdots a^\dagger_{a_A} a_{i_A}\cdots a_{i_1} .
\end{equation}
!et


Thus, the operator (ref{Top}) induces particle-hole (p-h)
excitations with respect to the reference. In general, $T$ generates
up to $Ap-Ah$ excitations, and the unknown parameters are the cluster amplitides
$t_i^a$, $t_{ij}^{ab}$, ..., $t_{i_1,\ldots,i_A}^{a_1,\ldots,a_A}$.

!bblock
===== Exercise: How many unknowns? =====
label{ex-6}
Show that the number of unknowns is as large as the FCI dimension of
the problem, using the numbers $A$ and $n_u$.

!bans

We have to sum up all $np-nh$ excitations, and there are
$\binom{n_u}{n}$ particle states and $\binom{A}{A-n}$ hole states for
each $n$. Thus, we have for the total number

!bt
\begin{align}
\sum_{n=0}^A \binom{n_u}{n} \binom{A}{A-n}= \binom{A+n_u}{A} .
\end{align}
!et

The right hand side are obviously all ways to distribute $A$ fermions over $n_0+A$ orbitals.
!eans
===== =====
!eblock


Thus, the coupled-cluster method with the full cluster operator
(ref{Top}) is exponentially expensive, just as FCI. To make progress,
we need to make an approximation by truncating the operator. Here, we
will use the CCSD (coupled clusters singles doubles) approximation,
where

!bt
\begin{equation}
\label{Tccsd}
T \equiv \sum_{ia} t_i^a a^\dagger_a a_i + \frac{1}{4}\sum_{ijab}t_{ij}^{ab}
a^\dagger_aa^\dagger_ba_ja_i .
\end{equation}
!et

We need to determine the unknown cluster amplitudes that enter in CCSD. Let

!bt
\begin{align}
\vert\Phi_i^a\rangle &= a^\dagger_a a_i \vert \Phi_0\rangle , \\
\vert\Phi_{ij}^{ab}\rangle &= a^\dagger_a a^\dagger_b a_j a_i \vert \Phi_0\rangle
\end{align}
!et

be 1p-1h and 2p-2h excitations of the reference. Computing matrix
elements of the Schroedinger Equation (ref{Schroedinger}) yields

!bt
\begin{align}
\label{ccsd}
\langle \Phi_0\vert \overline{H_N}\vert \Phi_0\rangle &= E_c , \\
\langle \Phi_i^a\vert \overline{H_N}\vert \Phi_0\rangle &= 0 , \\
\langle \Phi_{ij}^{ab}\vert \overline{H_N}\vert \Phi_0\rangle &= 0 .
\end{align}
!et

The first equation states that the coupled-cluster correlation energy
is an expectation value of the similarity-transformed Hamiltonian. The
second and third equations state that the similarity-transformed
Hamiltonian exhibits no 1p-1h and no 2p-2h excitations. These
equations have to be solved to find the unknown amplitudes $t_i^a$ and
$t_{ij}^{ab}$. Then one can use these amplitudes and compute the
correlation energy from the first line of Eq. (ref{ccsd}).

We note that in the CCSD approximation the reference state is not an
exact eigenstates. Rather, it is decoupled from simple states but
$\overline{H}$ still connects this state to 3p-3h, and 4p-4h states
etc.

At this point, it is important to recall that we assumed starting from
a ``good'' reference state. In such a case, we might reasonably expect
that the inclusion of 1p-1h and 2p-2h excitations could result in an
accurate approximation. Indeed, empirically one finds that CCSD
accounts for about 90% of the corelation energy, i.e. of the
difference between the exact energy and the Hartree-Fock energy. The
inclusion of triples (3p-3h excitations) typically yields 99% of the
correlation energy.

We see that the coupled-cluster method in its CCSD approximation
yields a similarity-transformed Hamiltonian that is of a two-body
structure with respect to a non-trivial vacuum. When viewed in this
light, the coupled-cluster method ``transforms'' an $A$-body problem
(in CCSD) into a two-body problem, albeit with respect to a nontrivial
vacuum.


!bblock
===== Exercise: Why is CCD not exact? =====
label{ex-6b}
Above we argued that a similarity transformation preserves all eigenvalues. Nevertheless, the CCD correlation energy is not the exact correlation energy. Explain!


!bans

The CCD approximation does not make $\vert\Phi_0\rangle$ an exact
eigenstate of $\overline{H_N}$; it is only an eigenstate when the
similarity-transformed Hamiltonian is truncated to at most 2p-2h
states. The full $\overline{H_N}$, with $T=T_2$, would involve
six-body terms (do you understand this?), and this full Hamiltonian
would reproduce the exact correlation energy. Thus CCD is a similarity
transformation plus a truncation, which decouples the ground state only
from 2p-2h states.

!eans
===== =====
!eblock

======= Computing the similarity-transformed Hamiltonian =======

The solution of the CCSD equations, i.e. the second and third line of
Eq. (ref{ccsd}), and the computation of the correlation energy
requires us to compute matrix elements of the similarity-transformed
Hamiltonian (ref{Hsim}). This can be done with the
Baker-Campbell-Hausdorff expansion

!bt
\begin{align}
\label{BCH}
\overline{H_N} &= e^{-T} H_N e^T \\
&=H_N + \left[ H_N, T\right]+ \frac{1}{2!}\left[ \left[ H_N, T\right], T\right]
+ \frac{1}{3!}\left[\left[ \left[ H_N, T\right], T\right], T\right] +\ldots .
\end{align}
!et

We now come to a key element of coupled-cluster theory: the cluster
operator (ref{Top}) consists of sums of terms that consist of particle
creation and hole annihilation operators (but no particle annihilation
or hole creation operators). Thus, all terms that enter $T$ commute
with each other. This means that the commutators in the
Baker-Campbell-Hausdorff expansion (ref{BCH}) can only be non-zero
because each $T$ must connect to $H_N$ (but no $T$ with another
$T$). Thus, the expansion is finite.

!bblock
===== Exercise: When does CCSD truncate? =====
label{ex-7}
In CCSD and for two-body Hamiltonians, how many nested
commutators yield nonzero results? Where does the
Baker-Campbell-Hausdorff expansion terminate? What is the (many-body) rank of the resulting $\overline{H_N}$? 

!bans

CCSD truncates for two-body operators at four-fold nested commutators,
because each of the four annihilation and creation operators in
$\overline{H_N}$ can be knocked out with one term of $T$.

!eans
===== =====
!eblock


We see that the (disadvantage of having a) non-Hermitian Hamiltonian
$\overline{H_N}$ leads to the advantage that the
Baker-Campbell-Hausdorff expansion is finite, thus leading to the
possibility to compute $\overline{H_N}$ exactly. In contrast, the
IMSRG deals with a Hermitian Hamiltonian throughout, and the infinite
Baker-Campbell-Hausdorff expansion is truncated at a high order when
terms become very small.

We write the similarity-transformed Hamiltonian as

!bt
\begin{align}
\overline{H_N}=\sum_{pq} \overline{H}^p_q a^\dagger_q a_p + {1\over 4} \sum_{pqrs} \overline{H}^{pq}_{rs} a^\dagger_p a^\dagger_q a_s a_r + \ldots
\end{align}
!et
with
!bt
\begin{align}
\overline{H}^p_q &\equiv \langle p\vert \overline{H_N}\vert q\rangle , \\
\overline{H}^{pq}_{rs} &\equiv \langle pq\vert \overline{H_N}\vert rs\rangle .
\end{align}
!et

Thus, the CCSD Eqs.~(ref{ccsd}) for the amplitudes can be written as
$\overline{H}_i^a = 0$ and $\overline{H}_{ij}^{ab}=0$.

!bblock
===== Exercise: Compute the matrix element $\overline{H}_{ab}^{ij}\equiv \langle ij\vert \overline{H_N}\vert ab\rangle$ =====
label{ex-8}


!bans

This is a simple task. This matrix element is part of the operator
$\overline{H}_{ab}^{ij}a^\dagger_ia^\dagger_ja_ba_a$, i.e. particles
are annihilated and holes are created. Thus, no contraction of the
Hamiltonian $H$ with any cluster operator $T$ (remember that $T$
annihilates holes and creates particles) can happen, and we simply
have $\overline{H}_{ab}^{ij} = \langle ij\vert V\vert ab\rangle$.

!eans
===== =====
!eblock


We need to work out the similarity-transformed Hamiltonian of
Eq. (ref{BCH}). To do this, we write $T=T_1 +T_2$ and $H_N= F +V$,
where $T_1$ and $F$ are one-body operators, and $T_2$ and $V$ are
two-body operators.

!bblock
===== Example: The contribution of $[F, T_2]$ to $\overline{H_N}$ =====
label{ex-9}

The commutator $[F, T_2]$ consists of two-body and one-body terms. Let
us compute first the two-body term, as it results from a single
contraction (i.e. a single application of $[a_p, a^\dagger_q] =
\delta_p^q$). We denote this as $[F, T_2]_{2b}$ and find

!bt
\begin{align*}
[F, T_2]_{2b} &= \frac{1}{4}\sum_{pq}\sum_{rsuv} f_p^q t_{ij}^{ab}\left[a^\dagger_q a_p, a^\dagger_a a^\dagger_b a_j a_i \right]_{2b} \\
&= \frac{1}{4}\sum_{pq}\sum_{abij} f_p^q t_{ij}^{ab}\delta_p^a a^\dagger_q a^\dagger_b a_j a_i  \\
&- \frac{1}{4}\sum_{pq}\sum_{abij} f_p^q t_{ij}^{ab}\delta_p^b a^\dagger_q a^\dagger_a a_j a_i  \\
&- \frac{1}{4}\sum_{pq}\sum_{abij} f_p^q t_{ij}^{ab}\delta_q^j a^\dagger_a a^\dagger_b a_p a_i  \\
&+ \frac{1}{4}\sum_{pq}\sum_{abij} f_p^q t_{ij}^{ab}\delta_q^i a^\dagger_a a^\dagger_b a_p a_j  \\
&= \frac{1}{4}\sum_{qbij}\left(\sum_{a} f_a^q t_{ij}^{ab}\right)a^\dagger_q a^\dagger_b a_j a_i  \\
&- \frac{1}{4}\sum_{qaij}\left(\sum_{b} f_b^q t_{ij}^{ab}\right)a^\dagger_q a^\dagger_a a_j a_i  \\
&- \frac{1}{4}\sum_{pabi}\left(\sum_{j} f_p^j t_{ij}^{ab}\right)a^\dagger_a a^\dagger_b a_p a_i  \\
&+ \frac{1}{4}\sum_{pabj}\left(\sum_{i} f_p^i t_{ij}^{ab}\right)a^\dagger_a a^\dagger_b a_p a_j  \\
&= \frac{1}{2}\sum_{qbij}\left(\sum_{a} f_a^q t_{ij}^{ab}\right)a^\dagger_q a^\dagger_b a_j a_i  \\
&- \frac{1}{2}\sum_{pabi}\left(\sum_{j} f_p^j t_{ij}^{ab}\right)a^\dagger_a a^\dagger_b a_p a_i  .
\end{align*}
!et

Here we exploited the antisymmetry $t_{ij}^{ab} = -t_{ji}^{ab} =
-t_{ij}^{ba} = t_{ji}^{ba}$ in the last step. Using $a^\dagger_q a^\dagger_b a_j a_i = -a^\dagger_b a^\dagger_q a_j a_i $ and $a^\dagger_a a^\dagger_b a_p a_i = a^\dagger_a a^\dagger_b a_i a_p$, we can make the expression 
manifest antisymmetric, i.e.

!bt
\begin{align*}
[F, T_2]_{2b}
&= \frac{1}{4}\sum_{qbij}\left[\sum_{a} \left(f_a^q t_{ij}^{ab}-f_a^b t_{ij}^{qa}\right)\right]a^\dagger_q a^\dagger_b a_j a_i  \\
&- \frac{1}{4}\sum_{pabi}\left[\sum_{j} \left(f_p^j t_{ij}^{ab}-f_i^j t_{pj}^{ab}\right)\right]a^\dagger_a a^\dagger_b a_p a_i  .
\end{align*}
!et
Thus, the contribution of $[F, T_2]_{2b}$ to the matrix element $\overline{H}_{ij}^{ab}$ is  
!bt
\begin{align*}
\overline{H}_{ij}^{ab} \leftarrow  \sum_{c} \left(f_c^a t_{ij}^{cb}-f_c^b t_{ij}^{ac}\right) - \sum_{k} \left(f_j^k t_{ik}^{ab}-f_i^k t_{jk}^{ab}\right)
\end{align*}
!et

Here we used an arrow to indicate that this is just one contribution
to this matrix element.  We see that the derivation straight forward,
but somewhat tedious. As no one likes to commute too much (neither in
this example nor when going to and from work), and so we need a better
approach. This is where diagramms come in handy.

===== =====
!eblock


=== Diagrams  ===
The pictures in this Subsection are taken from Crawford and Schaefer.

By convention, hole lines (labels $i, j, k,\ldots$) are pointing down. 
FIGURE: [figslides/Diagram-i.png, height=120 frac=0.8] This is a hole line. label{fig-i}

By convention, particle lines (labels $a, b, c,\ldots$) are pointing up. 
FIGURE: [figslides/Diagram-a.png, height=120 frac=0.8] This is a particle line. label{fig-a}

Let us look at the one-body operator of the normal-ordered Hamiltonian, i.e. Fock matrix. Its diagrams are as follows.

FIGURE: [figslides/Diagram-fab.png, height=120 frac=0.8] The diagrams corresponding to $f_a^b$. The dashed line with the 'X' denotes the interaction $F$ between the incoming and outgoing lines. The labels $a$ and $b$ are not denoted, but you should label the outgoing and incoming lines accordingly. label{fig-fab}

FIGURE: [figslides/Diagram-fij.png, height=120 frac=0.8] The diagrams corresponding to $f_i^j$. The dashed line with the 'X' denotes the interaction $F$ between the incoming and outgoing lines. label{fig-fij}

FIGURE: [figslides/Diagram-fia.png, height=120 frac=0.8] The diagrams corresponding to $f_a^i$. The dashed line with the 'X' denotes the interaction $F$ between the incoming and outgoing lines. label{fig-fia}

FIGURE: [figslides/Diagram-fai.png, height=120 frac=0.8] The diagrams corresponding to $f_i^a$. The dashed line with the 'X' denotes the interaction $F$ between the incoming and outgoing lines. label{fig-fai}

We now turn to the two-body interaction. It is denoted as a horizontal
dashed line with incoming and outgoing lines attached to it. We start
by noting that the following diagrams of the interaction are all
related by permutation symmetry.

FIGURE: [figslides/Diagrams-symmetry.png, width=400 frac=0.8] The diagrams corresponding to $\langle ai\vert V\vert jb \rangle = - \langle ai\vert V\vert bj \rangle = -\langle ia\vert V\vert jb \rangle = \langle ia\vert V\vert bj\rangle$. label{fig-symmetry}

!bblock
===== Exercise: Assign the correct matrix element $\langle pq\vert V\vert rs\rangle$ to each of the following diagrams of the interaction =====
label{ex-10}

Remember: $\langle\rm{left-out, right-out}\vert V\vert \rm{left-in, right-in}\rangle$.

!bsubex
FIGURE: [figslides/Diagrams-V1.png, width=400 frac=0.8] 
label{fig-V1}

!bans
$\langle ab\vert V\vert cd\rangle + \langle ij\vert V\vert kl\rangle + \langle ia\vert V\vert bj\rangle$
!eans
!esubex

!bsubex
FIGURE: [figslides/Diagrams-V2.png, width=400 frac=0.8] 
label{fig-V2}

!bans
$\langle ai\vert V\vert bc\rangle + \langle ij\vert V\vert ka\rangle + \langle ab\vert V\vert ci\rangle$
!eans
!esubex

!bsubex
FIGURE: [figslides/Diagrams-V3.png, width=400 frac=0.8] 
label{fig-V3}

!bans
$\langle ia\vert V\vert jk\rangle + \langle ab\vert V\vert ij\rangle + \langle ij\vert V\vert ab\rangle$
!eans
!esubex

===== =====
!eblock


Finally, we have the following diagrams for the $T_1$ and $T_2$ amplitudes.
FIGURE: [figslides/Diagrams-T.png, width=400 frac=0.8] The horizontal full line is the cluster amplitude with incoming hole lines and outgoing particle lines as indicated. label{fig-T}

We are now in the position to construct the diagrams of the
similarity-transformed Hamiltonian, keeping in mind that these
diagrams correspond to matrix elements of $\overline{H_N}$. The rules
are as follows.

o Write down all *topologically different* diagrams corresponding to the desired matrix element. Topologically different diagrams differ in the number and type of lines (particle or hole) that connect the Fock matrix $F$ or the interaction $V$ to the cluster amplitudes $T$, but not whether these connections are left or right (as those are related by antisymmetry). As an example, all diagrams in Fig. ref{fig-symmetry} are topologically identical, because they consist of incoming particle and hole lines and of outgoing particle and hole lines. 

o Write down the matrix elements that enter the diagram, and sum over all internal lines. 

o The overall sign is $(-1)$ to the power of [(number of hole lines) -- (number of loops)].

o Symmetry factor: For each pair of equivalent lines (i.e. lines that connect the same two operators) multiply with a factor $1/2$. For $n$ identical vertices, multiply the algebraic expression by the symmery factor $1/n!$ to account properly for the number of ways the diagram can be constructed.   

o Antisymmetrize the outgoing and incoming lines as necessary.

Please note that this really works. You could derive these rules for
yourself from the commutations and factors that enter the
Baker-Campbell-Hausdorff expansion. The sign comes obviously from the
arrangement of creation and annilhilation operators, while the
symmetry factor stems from all the different ways, one can contract
the cluster operator with the normal-ordered Hamiltonian.


!bblock
===== Example: CCSD correlation energy =====
label{ex-11}
The CCSD correlation energy, $E_c= \langle
\Phi_0\vert \overline{H_N}\vert \Phi_0\rangle$, is the first of the CCSD
equations (ref{ccsd}). It is a vacuum expectation value and thus
consists of all diagrams with no external legs. There are three such diagrams:

FIGURE: [figslides/Diagrams-E0.png, width=400 frac=0.8] Three diagrams enter for the CCSD correlation energy, i.e. all diagrams that leave no external legs. label{fig-E0}

The correponding algebraic expression is $E_c=\sum_{ia}f^i_a t_i^a +{1\over 4}\sum_{ijab} \langle ij\vert V\vert ab\rangle t_{ij}^{ab} + {1\over 2} \sum_{ijab} \langle ij\vert V\vert ab\rangle t_i^a t_j^b$.

The first algebraic expression is clear. We have one hole line and one
loop, giving it a positive sign. There are no equivalent lines or
vertices, giving it no symmetry factor. The second diagram has two
loops and two hole lines, again leading to a positive sign. We have a
pair of equivalent hole lines and a pair of equivalent particle lines,
each giving a symmetry factor of $1/2$. The third diagram has two
loops and two hole lines, again leading to a positive sign. We have
two indentical vertices (each connecting to a $T_1$ in the same way)
and thus a symmetry factor $1/2$.

===== =====
!eblock


======= CCD Approximation =======

In what follows, we will consider the coupled cluster doubles (CCD)
approximation. This approximation is valid in cases where the system
cannot exhibit any particle-hole excitations (such as nuclear matter
when formulated on a momentum-space grid) or for the pairing model (as
the pairing interactions only excites pairs of particles). In this
case $t_i^a=0$ for all $i, a$, and $\overline{H}_i^a=0$. The CCD
approximation is also of some sort of leading order approximation in
the Hartree-Fock basis (as the Hartree-Fock Hamiltonian exhibits no
particle-hole excitations).


!bblock
===== Exercise: Derive the CCD equations! =====
label{ex-12}
Let us consider the matrix element $\overline{H}_{ij}^{ab}$. Clearly,
it consists of all diagrams (i.e. all combinations of $T_2$, and a
single $F$ or $V$ that have two incoming hole lines and two outgoing
particle lines. Write down all these diagrams.

!bhint
Start systematically! Consider all combinations of $F$ and $V$ diagrams with 0, 1, and 2 cluster amplitudes $T_2$.
!ehint

!bans

FIGURE: [figslides/Diagrams-CCD.png, width=400 frac=0.8] The diagrams for the $T_2$ equation, i.e. the matrix elements of $\overline{H}_{ij}^{ab}$. Taken from Baardsen et al (2013). label{fig-ccd}

The corresponding algebraic expression is
!bt
\begin{align*}
\overline{H}_{ij}^{ab} &= \langle ab\vert V\vert ij\rangle + P(ab)\sum_c f_c^bt_{ij}^{ac} - P(ij)\sum_k f_j^k t_{ik}^{ab} \\
&+ {1\over 2} \sum_{cd} \langle ab\vert V\vert cd\rangle t_{ij}^{cd}+ {1\over 2} \sum_{kl} \langle kl\vert V\vert ij\rangle t_{kl}^{ab} + P(ab)P(ij)\sum_{kc} \langle kb\vert V\vert cj \rangle t_{ik}^{ac} \\
&+ {1\over 2} P(ij)P(ab)\sum_{kcld} \langle kl\vert V\vert cd\rangle t_{ik}^{ac}t_{lj}^{db} 
+ {1\over 2} P(ij)\sum_{kcld} \langle kl\vert V\vert cd\rangle t_{ik}^{cd}t_{lj}^{ab}\\
&+ {1\over 2} P(ab)\sum_{kcld} \langle kl\vert V\vert cd\rangle t_{kl}^{ac}t_{ij}^{db}
+ {1\over 4} \sum_{kcld} \langle kl\vert V\vert cd\rangle t_{ij}^{cd}t_{kl}^{ab} . 
\end{align*}
!et

!eans

===== =====
!eblock


Let us now turn to the computational cost of a CCD computation.

!bblock
===== Exercise: Computational scaling of CCD =====
label{ex-13}
For each of the diagrams in Fig. ref{fig-ccd} write down the
computational cost in terms of the number of occupied $A$ and the
number of unoccupied $n_u$ orbitals.

!bans
The cost is $A^2 n_u^2$, $A^2 n_u^3$, $A^3 n_u^2$,
$A^2 n_u^4$, $A^4 n_u^2$, $A^3 n_u^3$,
$A^4 n_u^4$, $A^4 n_u^4$,
$A^4 n_u^4$, and $A^4 n_u^4$ for the respective diagrams.
!eans

===== =====
!eblock

Note that $n_u\gg A$ in general. In textbooks, one reads that CCD (and
CCSD) cost only $A^2n_u^4$. Our most expensive diagrams, however are
$A^4n_u^4$. What is going on?

To understand this puzzle, let us consider the last diagram of
Fig. ref{fig-ccd}. We break up the computation into two steps,
computing first the intermediate
!bt
\begin{align}
\chi_{ij}^{kl}\equiv {1\over 2} \sum_{cd} \langle kl\vert V\vert cd\rangle t_{ij}^{cd}
\end{align}
!et
at a cost of $A^4n_u^2$, and then 
!bt
\begin{align}
{1\over 2} \sum_{kl} \chi_{ij}^{kl} t_{kl}^{ab}  
\end{align}
!et
at a cost of $A^4n_u^2$. This is affordable. The price to pay is the
storage of the intermediate $\chi_{ij}^{kl}$, i.e. we traded
memory for computational cycles. This trick is known as
``factorization.'' 


!bblock
===== Exercise: Factorize the remaining diagrams of the CCD equation =====
label{ex-14}
Diagrams 7, 8, and 9 of Fig. ref{fig-ccd} also need to be factorized. 


!bans
For diagram number 7, we compute
!bt
\begin{align}
\chi_{id}^{al}\equiv\sum_{kc} \langle kl\vert V\vert cd\rangle t_{ik}^{ac}
\end{align}
!et
at a cost of $A^3 n_u^3$ and then compute
!bt
\begin{align}
{1\over 2} P(ij)P(ab) \sum_{ld} \chi_{id}^{al} t_{lj}^{db} 
\end{align}
!et
at the cost of $A^3 n_u^3$.

For diagram number 8, we compute
!bt
\begin{align}
\chi_{i}^{l}\equiv -{1\over 2} \sum_{kcd} \langle kl\vert V\vert cd\rangle t_{ik}^{cd}
\end{align}
!et
at a cost of $A^3 n_u^2$, and then compute
!bt
\begin{align}
-P(ij) \sum_l \chi_i^l t_{lj}^{ab}
\end{align}
!et
at the cost of $A^3 n_u^2$.

For diagram number 9, we compute
!bt
\begin{align}
\chi_d^a\equiv{1\over 2} \sum_{kcl} \langle kl\vert V\vert cd\rangle t_{kl}^{ac}
\end{align}
!et
at a cost of $A^2 n_u^3$ and then compute
!bt
\begin{align}
P(ab)\sum_d \chi_d^a t_{ij}^{db}
\end{align}
!et
at the cost of $A^3 n_u^3$.
!eans

===== =====
!eblock


We are now ready, to derive the full CCSD equations, i.e. the matrix
elements of $\overline{H}_i^a$ and $\overline{H}_{ij}^{ab}$. 


!bblock
===== Project: (Optional) Derive the CCSD equations! =====
label{ex-15}

!bsubex
Let us consider the matrix element $\overline{H}_i^a$ first. Clearly, it consists of all diagrams (i.e. all combinations of $T_1$, $T_2$, and a single $F$ or $V$ that have an incoming hole line and an outgoing particle line. Write down all these diagrams. 

!bans

FIGURE: [figslides/Diagrams-CCSD1.png, width=800 frac=0.8] The diagrams for the $T_1$ equation, i.e. the matrix elements of $\overline{H}_i^a$. Taken from Crawford and Schaefer. Here $\langle pq\vert \vert rs\rangle \equiv \langle pq\vert V\vert rs\rangle$ and $f_{pq}\equiv f^p_q$. label{fig-CCSD1}

!eans
!esubex

!bsubex
Let us now consider the matrix element $\overline{H}_{ij}^{ab}$. Clearly, it consists of all diagrams (i.e. all combinations of $T_1$, $T_2$, and a single $F$ or $V$ that have two incoming hole lines and two outgoing particle lines. Write down all these diagrams and corresponding algebraic expressions. 

!bans

FIGURE: [figslides/Diagrams-CCSD2.png, width=800 frac=0.8] The diagrams for the $T_2$ equation, i.e. the matrix elements of $\overline{H}_{ij}^{ab}$. Taken from Crawford and Schaefer. Here $\langle pq\vert \vert rs\rangle \equiv \langle pq\vert V\vert rs\rangle$, $f_{pq}\equiv f^p_q$, and $P(ab) = 1 - (a\leftrightarrow b)$ antisymmetrizes. label{fig-CCSD2}

!eans
!esubex

===== =====
!eblock

We can now turn to the solution of the coupled-cluster equations.



===== Solving the CCD equations =====

The CCD equations, depicted in Fig. ref{fig-ccd}, are nonlinear in the
cluster amplitudes. How do we solve $\overline{H}_{ij}^{ab}=0$? We
subtract $(f_a^a +f_b^b -f_i^i -f_j^j)t_{ij}^{ab}$ from both sides of
$\overline{H}_{ij}^{ab}=0$ (because this term is contained in
$\overline{H}_{ij}^{ab}$) and find

!bt
\begin{align*}
(f_i^i +f_j^j -f_a^a -f_b^b)t_{ij}^{ab} &= (f_i^i +f_j^j -f_a^a -f_b^b)t_{ij}^{ab} +\overline{H}_{ij}^{ab}
\end{align*}
!et

Dividing by $(f_i^i +f_j^j -f_a^a -f_b^b)$ yields
!bt
\begin{align}
t_{ij}^{ab} &= t_{ij}^{ab} + \frac{\overline{H}_{ij}^{ab}}{f_i^i +f_j^j -f_a^a -f_b^b}
\label{iter}
\end{align}
!et

This equation is of the type $t=f(t)$, and we solve it by iteration,
i.e. we start with a guess $t_0$ and iterate $t_{n+1}=f(t_n)$, and
hope that this will converge to a solution. We take the perturbative result
!bt
\begin{align}
\label{pert}
\left(t_{ij}^{ab}\right)_0 = \frac{\langle ab\vert V\vert ij\rangle}{f_i^i +f_j^j -f_a^a -f_b^b}
\end{align}
!et

as a starting point, compute $\overline{H}_{ij}^{ab}$, and find a new
$t_{ij}^{ab}$ from the right-hand side of Eq. (ref{iter}). We repeat
this process until the amplitudes (or the CCD energy) converge.



======= CCD for the pairing Hamiltonian =======


You learned about the pairing Hamiltonian earlier in this
school. Convince yourself that this Hamiltonian does not induce any
1p-1h excitations. Let us solve the CCD equations for this
problem. This consists of the following steps

o Write a function that compute the potential, i.e. it returns a four-indexed array (or tensor). We need $\langle ab\vert V\vert cd\rangle$, $\langle ij\vert V\vert kl\rangle$, and $\langle ab\vert V\vert ij\rangle$. Why is there no $\langle ab\vert V\vert id\rangle$ or $\langle ai\vert V\vert jb\rangle$ ?

o Write a function that computes the Fock matrix, i.e. a two-indexed array. We only need $f_a^b$ and $f_i^j$. Why? 

o Initialize the cluster amplitudes according to Eq. (ref{pert}), and solve Eq. (ref{iter}) by iteration. The cluster amplitudes $T_1$ and $T_2$ are two- and four-indexed arrays, respectively.

Please note that the contraction of tensors (i.e. the summation over
common indices in products of tensors) is very user friendly and
elegant in python when `numpy.einsum` is used.




Classical many-body methods in quantum mechanics often trade
computational complexity for accuracy when solving for the ground
state energy of a system. The recent advances in quantum computing
have shown promise in approximating the ground state energy without
such a large trade-off \cite{QC1}. Hence, we will look into a couple
of promising methods in this section.  Before we start learning about
how quantum computing works, we will give a short introduction to the
building blocks of classical computers. In classical computing, the
basic unit of information is called a bit. The bit is represented by a
binary digit, either a 1 or a 0. A collection of bits provides a
binary string with information, and this information can be
manipulated with what we call logic gates. Logic gates are operations
on one or more bits and produce a single output, 1 or 0. A collection
of such logic gates is called a circuit and an example of such a
circuit could be outputting a 1 if all input bits are put to 1 and
output 0 otherwise. The behaviour of all circuits on classical
computers is deterministic in the sense that a given input binary
string will always produce the same output.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.1]{figures/methods/BlockSphere.png}
    \caption{The Block sphere. Figure was found in reference \cite{BlockSphere}}
    \label{fig:BlockSphere}
\end{figure}

How quantum computing differs from classical computing can partly be illustrated by looking at the surface of the unit sphere in figure \ref{fig:BlockSphere}, which is commonly referred to as the Block sphere. Whereas a classical bit must either be in the $\ket{0}$ state at the surface on the north pole, or in the $\ket{1}$ state at the surface on the south pole; the quantum bit, namely a qubit, can be in a state located anywhere on the surface of the Block sphere. The qubit state $\ket{\psi}$ can be in what we know from quantum mechanics as a superposition, 
$$ \ket{\psi} = c_0 \ket{0} + c_1 \ket{1},$$
where a measurement of the qubit will collapse the state, resulting in the state $\ket{0}$ or $\ket{1}$ with probability $|c_0|^2$ or $|c_1|^2$ respectively. These coefficients are referred to as amplitudes and could be any complex number as long as the measurement probabilities are normalized, that is $|c_0|^2 + |c_1|^2 = 1$. The orientation on the $x,y$ plane of the surface is given by the complex phases. There are also other quantum mechanical properties than superposition that apply to the qubits, namely entanglement. Just like particles can become entangled, several qubits can become entangled, meaning that the measurement outcome of one qubit can directly affect another. How the properties of superposition and entanglement have the potential to be used for our advantage will be shown during this chapter. But first, we will begin by introducing the basis states we are dealing with in quantum computing.

\section{Introduction to Quantum Computing}
\label{sec:QCIntroduction}
Our introduction to quantum computing is mostly based on the book \textit{Quantum Computation and Quantum Information} by Michael A. Nielsen & Isaac L. Chuang \cite{NielsenAndChuang}. It is an excellent read and we highly recommend it if new to quantum computing.
\subsection{Basis}
\label{subsec:QCBasis}

In order to start understanding how to manipulate one or several qubits to our advantage, we need to specify the basis we are working in. This is important as the manipulation of a qubit can mathematically be represented by matrix-vector multiplications. As we have talked about earlier, the measurement of a qubit will result in either the $\ket{0}$ or the $\ket{1}$ state, and it is these states which form the computational basis for quantum computing. We can write them in vector form as
\begin{equation}
    \label{eq:QCChapComputationalBasisStates}
    \ket{0} \equiv \begin{bmatrix} 1 \\ 0
    \end{bmatrix}
    \qquad \text{and} \qquad 
    \ket{1} \equiv \begin{bmatrix} 0 \\ 1
    \end{bmatrix},
\end{equation}
and it is important to note that any linear combination of these states is an allowed state of a qubit, as long as the unit norm is preserved. The preservation of unit norm serves as a clue as to what sort of operations we are allowed to perform on a qubit. Consider $U$ to be a unitary matrix, that is
\begin{equation*}
    \label{eq:QCChapUnitaryMatrix}
    U^\dagger U = UU^\dagger = I,
\end{equation*}
where $I$ is the identity matrix. Now consider an arbitrary qubit state $\ket{\psi}$ with unit norm, that is $\bra{\psi} \ket{\psi} = 1$.
The action of $U$ on this state gives
$$U \ket{\psi} = \ket{\phi}.$$
The norm of the transformed state is then
\begin{equation*}
    \label{eq:QCCHapUnitaryPreserveNorm}
    \bra{\phi} \ket{\phi} = \bra{\psi} \underbrace{U^\dagger U}_{= I}\ket{\psi} = \bra{\psi} \ket{ \psi} = 1.
\end{equation*}
Hence, any unitary transformation preserves the norm of the qubit state. This is no coincidence as the second postulate of quantum mechanics states that the evolution of a closed quantum system is described by a unitary transformation \cite{NielsenAndChuang}. Unitary transformations are the quantum equivalent of classical logic gates, and hence will often be referred to as gates throughout this thesis. 
A system of multiple qubits are represented by tensor products. For example, for an $n$-qubit state we may write
\begin{equation}
    \label{eq:TensorProduct}
    \ket{\psi_1}\ket{\psi_2}\cdots \ket{\psi_n} \equiv \ket{\psi_1 \psi_2 \cdots \psi_n} \equiv \ket{\psi_1} \otimes \ket{\psi_2} \otimes \cdots \otimes \ket{\psi_n}.
\end{equation}
We may also split the qubits into two or more sub-systems, which we will call registers, as this could be convenient when explaining the algorithms. An example is the state
\begin{equation*}
    \ket{p}\ket{q},
\end{equation*}
where $\ket{p}$ and $\ket{q}$ are $m$ and $n$-qubit states, respectively.
We can represent manipulations on multi-qubit states by a tensor product of unitary operators;
\begin{align}
    \label{eq:QuantumGates}
    (A \otimes B \otimes \cdots \otimes N ) \ket{\psi_1} \otimes \ket{\psi_2} \otimes \cdots \otimes \ket{\psi_n} &= A\ket{\psi_1} \otimes B\ket{\psi_2} \otimes \cdots \otimes N \ket{\psi_n}, \notag \\
    &\text{or} \notag \\
    A^1 B^2\cdots N^n \ket{\psi_1}\ket{\psi_2}\cdots \ket{\psi_n} &= A^1\ket{\psi_1}B^2\ket{\psi_2}\cdots N^n\ket{\psi_n},
\end{align}
where the superscript denotes which qubit the operator acts on.
We will now go through some of the most common gates we deal with on a quantum computer. 

\subsection{Quantum Gates}
\label{subsec:QCQuantumGates}

The first set of gates introduced are what we call single qubit gates. Single qubit gates, as you may have guessed from the name, are gates that only act on a single qubit. Since a qubit is represented by a two-dimensional vector, the single qubit gates can be represented by any two-by-two unitary matrix. Even though any such matrices transforms our qubit into a valid state, there are some notable gates that are important for many of the applications we will consider. First up we have
\begin{equation*}
    X = \sigma_x = \begin{bmatrix} 0 & 1 \\ 1 & 0
    \end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{X}    & \qw  \\
}
\end{array}
\end{equation*}
\begin{equation*}
     Y  = \sigma_y = \begin{bmatrix} 0 & -i \\ i & 0 
    \end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{Y}    & \qw  \\
}
\end{array}
\end{equation*}
\begin{equation}
    \label{eq:PauliMatrices}
    Z = \sigma_z = \begin{bmatrix} 1 & 0 \\ 0 & -1
    \end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{Z}    & \qw  \\
}
\end{array},
\end{equation}
which are referred to as the Pauli-$X$, Pauli-$Y$ and Pauli-$Z$ gates, or just $X$, $Y$ and $Z$ gates We have also included the circuit notation for the gates on the far right. We will go through more details on this in the next section.
The $X$ gate, for example, flips the qubit. That is
\begin{equation*}
    \sigma_x \ket{0} = \begin{bmatrix}
    0 & 1  \\
    1 & 0
\end{bmatrix} \begin{bmatrix}
    1 \\
    0
\end{bmatrix} = \begin{bmatrix}
    0 \\
    1
\end{bmatrix} = \ket{1}
\qquad \sigma_x \ket{1} = \begin{bmatrix}
    0 & 1  \\
    1 & 0
\end{bmatrix} \begin{bmatrix}
    0 \\
    1
\end{bmatrix} = \begin{bmatrix}
    1 \\
    0
\end{bmatrix} = \ket{0}.
\end{equation*}
We also have the Hadamard gate;
\begin{equation}
    \label{eq:HadamardGate}
    H = \frac{1}{\sqrt{2}} \begin{bmatrix}
    1 & 1  \\
    1 & -1
\end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{H}    & \qw  \\
}
\end{array},
\end{equation}
which creates a superposition:
\begin{align*}
    H \ket{0} = \frac{1}{\sqrt{2}}\begin{bmatrix}
    1 & 1  \\
    1 & -1
\end{bmatrix} \begin{bmatrix}
    1 \\
    0
\end{bmatrix} = \frac{1}{\sqrt{2}}\begin{bmatrix}
    1 \\
    1
\end{bmatrix} = \frac{1}{\sqrt{2}}(\ket{0} + \ket{1}) \\
H \ket{1} = \frac{1}{\sqrt{2}}\begin{bmatrix}
    1 & 1  \\
    1 & -1
\end{bmatrix} \begin{bmatrix}
    0 \\
    1
\end{bmatrix} =\frac{1}{\sqrt{2}} \begin{bmatrix}
    1 \\
    -1
\end{bmatrix} = \frac{1}{\sqrt{2}}(\ket{0} - \ket{1}).
\end{align*}
We can also rotate the qubit an arbitrary angle $\theta$ about the $x$, $y$ or $z$ axis on the Block sphere (figure \ref{fig:BlockSphere}). The gates that allow us to do this are called rotation gates, and they are subfixed with the rotation axis:
\begin{align}
    \label{eq:RotationOps}
     R_x(\theta) = e^{-i \theta \sigma_x /2} = \cos (\frac{\theta}{2})I - i \sin (\frac{\theta}{2})\sigma_x = \begin{bmatrix}
    \cos \frac{\theta}{2} & -i \sin \frac{\theta}{2}  \\
    -i \sin \frac{\theta}{2} & \cos \frac{\theta}{2}
\end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{R_x(\theta)}    & \qw  \\
}
\end{array} \notag \\
    R_y(\theta) = e^{-i \theta \sigma_y /2} = \cos (\frac{\theta}{2})I - i \sin (\frac{\theta}{2})\sigma_y = \begin{bmatrix}
    \cos \frac{\theta}{2} & -\sin \frac{\theta}{2}  \\
     \sin \frac{\theta}{2} & \cos \frac{\theta}{2}
\end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{R_y(\theta)}    & \qw  \\
}
\end{array} \\
    R_z(\theta) = e^{-i \theta \sigma_z /2} = \cos (\frac{\theta}{2})I - i \sin (\frac{\theta}{2})\sigma_z = \begin{bmatrix}
    e^{-i\theta /2} & 0  \\
    0 & e^{i \theta /2}
\end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{R_z(\theta)}    & \qw  \\
}
\end{array}. \notag
\end{align}
\bigskip
We also have a gate that applies a phase of $-i$ to the $\ket{1}$ state. It is called the phase shift gate and is given by
\begin{equation}
    \label{eq:Sgate}
    S \equiv \begin{bmatrix}
    1 & 0 \\
    0 & -i
\end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{S}    & \qw  \\
}
\end{array}.
\end{equation}
A gate that will become useful when we get into the quantum Fourier transform is the $R_k$ gate:
\begin{equation}
    \label{eq:Rkgate}
    R_k = \begin{bmatrix}
    1 & 0  \\
    0 & e^{2\pi i / 2^k}
\end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{R_k}    & \qw  \\
}
\end{array}.
\end{equation}
The first two-qubit gate we will introduce is the CNOT gate. Its matrix representation is given by
\begin{equation}
    \label{eq:CNOTMatrix}
    CNOT \equiv \begin{bmatrix}
    1 & 0 & 0 & 0  \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0
\end{bmatrix} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \ctrl{1}    & \qw  \\
& \targ & \qw \\
}
\end{array}.
\end{equation}
Operating on two qubits $\ket{c}\ket{t}$, its task is to flip the target qubit $\ket{t}$ if the control qubit $\ket{c}$ is in the $\ket{1}$-state. If it is in the $\ket{0}$-state it should act as the identity operator.
This kind of gate is called a controlled gate. In the case of the CNOT gate we perform a Pauli-$X$ gate (eq. (\ref{eq:PauliMatrices})) on the target qubit conditioned on the control qubit, but we can of course apply any of the other gates mentioned. Mathematically we will write down such a gate as
$$
X^c_t\ket{c}\ket{t},
$$
where the superfix denotes the control qubit, while the subfix denotes the target qubit.
The final gate we will mention is the Swap-operation. Given the $n$-qubit state 
$$\ket{\psi_1}\ket{\psi_2}\cdots \ket{\psi_i}\cdots \ket{\psi_j} \cdots \ket{\psi_n},$$
the swap gate applied to qubit $i$ and $j$ simply produces the state
\begin{equation}
    \label{eq:SwapOperation}
    \text{SWAP}_i^j \ket{\psi_1}\ket{\psi_2}\cdots \ket{\psi_i}\cdots \ket{\psi_j} \cdots \ket{\psi_n} = \ket{\psi_1}\ket{\psi_2}\cdots \ket{\psi_j}\cdots \ket{\psi_i} \cdots \ket{\psi_n}
\end{equation}
Hence, it puts the $j$'th qubit in the $i$'th qubit state and vise versa.
$$ \text{SWAP} \equiv  \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \qswap    & \qw  \\
& \qswap \qwx & \qw \\
}
\end{array}$$

\subsection{Quantum Circuits}
\label{subsec:QuantumCircuits}

A convenient way to write a quantum algorithm is through quantum circuits. A quantum cirquit consists of wires, where each wire represents a qubit:
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \qw & \qw      & \qw      & \qw     & \qw  & \qw & \qw & \qw & \qw & \qw  \\
& \qw      & \qw & \qw      & \qw     & \qw  & \qw & \qw & \qw &\qw & \qw  \\
& \qw      & \qw      & \qw & \qw     & \qw  & \qw & \qw & \qw&\qw & \qw  \\
& \qw      & \qw      & \qw      & \qw & \qw  & \qw & \qw & \qw&\qw & \qw \\
& \qw    & \qw   & \qw    & \qw   & \qw & \qw &\qw &\qw &\qw & \qw \\
}
\end{array}$$
These wires are not physical wires, but you can think of them as representing the passage of time. When reading a quantum circuit, the leftmost operations are performed first, so the circuits are read from left to right. All the qubits are usually initialized in the $\ket{0}$ state unless else is specified. To illustrate that an operation is performed on a qubit, we draw a gate on the wire corresponding to this qubit:
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{H}    & \qw  \\
}
\end{array}$$
In the above circuit, we only have one qubit which is put in superposition by applying the Hadamard gate (eq. (\ref{eq:HadamardGate})). To illustrate conditional operations, like the CNOT gate (eq. (\ref{eq:CNOTMatrix})), we write the circuit as follows:

$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \ctrl{1}    & \qw  \\
& \targ & \qw \\
}
\end{array} \equiv \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \ctrl{1}    & \qw  \\
& \gate{X} & \qw \\
}
\end{array}$$
The black dot indicates that we put the condition on the first qubit being in the $\ket{1}$ state, while the plus sign surrounded by a circle indicates which qubit to apply the $X$ gate (eq. (\ref{eq:PauliMatrices})) on. We refer to the conditional qubit as the control-qubit, whereas we refer to the qubit for which to eventually perform the operation on as the target qubit. We can also indicate conditional operations with an arbitrary gate $A$ as follows:
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \ctrl{1}    & \qw  \\
& \gate{A} & \qw \\
}
\end{array}$$
If we want to indicate that we perform a measurement on the bottom qubit in the circuit above, we can use a meter symbol on the wire corresponding to the qubit we measure:
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \ctrl{1}    & \qw  \\
& \gate{A} & \meter \\
}
\end{array}$$
The measurement of a qubit will either result in a 1 or a 0.

There are also multi-controlled qubit gates with more than one control qubit. For example, an operation conditional on three qubits can be written as
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \ctrl{3}    & \qw  \\
& \ctrlo{2} & \qw \\
& \ctrl{1} & \qw \\
& \gate{A} & \qw \\
}
\end{array}$$
You might have noticed that we used a white dot on the second qubit in this scenario. The white dot indicates that we condition on the respective qubit being in the $\ket{0}$ state instead of the $\ket{1}$ state. This is equivalent to acting on the qubit with an $X$ gate (eq. (\ref{eq:PauliMatrices})) before and after the multi-controlled operation, as the $X$ gate flips a qubit from the $\ket{0}$ state to the $\ket{1}$ state and vise versa:
$$
\begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \ctrl{3}    & \qw  \\
& \ctrlo{2} & \qw \\
& \ctrl{1} & \qw \\
& \gate{A} & \qw \\
}
\end{array}
$$
$$
\equiv
$$
$$
\Qcircuit @C=2em @R=1em {
&\qw & \ctrl{3}    & \qw & \qw  \\
& \gate{X} & \ctrl{2} & \gate{X}& \qw \\
& \qw & \ctrl{1} & \qw& \qw \\
& \qw & \gate{A} & \qw & \qw\\
} 
$$
It is relatively easy to read such circuits once the basics are down. However, before diving into the advanced circuits it is a good idea to go through a basic one.

\subsubsection{Coin toss example}
\label{subsubsec:CoinTossExample}

Say you want to calculate the probability of flipping heads on a coin four times in a row. As we know, each throw has a $50\%$ probability of producing a heads. To get a qubit to mimic a coin toss, we could put it in a superposition with equal probability of measuring the $\ket{0}$ and $\ket{1}$ state. We then refer to the $\ket{1}$ state as a heads and the $\ket{0}$ state as a tails. That is
$$\ket{\psi} = \frac{1}{\sqrt{2}}\ket{0} + \frac{1}{\sqrt{2}}\ket{1}. $$
As we can see, the probability of measuring the $\ket{0}$ state is given by
$$|\bra{0} \ket{\psi}|^2 = |\frac{1}{\sqrt{2}}\bra{0}\ket{0}|^2 = \frac{1}{2} ,$$
and likewise for the $\ket{1}$ state:
$$|\bra{1} \ket{\psi}|^2 = \frac{1}{2}.$$
As we learned earlier, the Hadamard gate (eq. (\ref{eq:HadamardGate})) produces such a state. The circuit for producing a single coin flip is then
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{H}    & \meter  \\
}
\end{array}$$
We could actually use this circuit for our purpose by repeatedly running it four times and counting how many times we get the $\ket{1}$ state in all four runs. However, for educational purposes we will include some more bells and whistles here. Let us instead use a single qubit for each coin toss in our experiment, and also a qubit to save the desired result:
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
c_1& & \qw & \qw      & \qw      & \qw     & \qw  & \qw & \qw & \qw & \qw & \qw  \\
c_2& & \qw      & \qw & \qw      & \qw     & \qw  & \qw & \qw & \qw &\qw & \qw  \\
c_3& & \qw      & \qw      & \qw & \qw     & \qw  & \qw & \qw & \qw&\qw & \qw  \\
c_4& & \qw      & \qw      & \qw      & \qw & \qw  & \qw & \qw & \qw&\qw & \qw \\
a& & \qw    & \qw   & \qw    & \qw   & \qw & \qw &\qw &\qw &\qw & \qw \\
}
\end{array}$$
The qubits denoted with a $c_i$ are the simulation qubits, that is, they are used to simulate the four coin tosses. The qubit denoted with an $a$ will be referred to as the ancilla qubit, which is a common name for qubits used to for example encode simulation results. Remember, all the qubits are initialized in the $\ket{0}$ state since nothing else is specified.
We start by putting all the simulation qubits in superpositions with the Hadamard gate (eq. (\ref{eq:HadamardGate})):
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
c_1& & \gate{H} & \qw      & \qw      & \qw     & \qw  & \qw & \qw & \qw & \qw & \qw  \\
c_2& & \gate{H}      & \qw & \qw      & \qw     & \qw  & \qw & \qw & \qw &\qw & \qw  \\
c_3& & \gate{H}      & \qw      & \qw & \qw     & \qw  & \qw & \qw & \qw&\qw & \qw  \\
c_4& & \gate{H}      & \qw      & \qw      & \qw & \qw  & \qw & \qw & \qw&\qw & \qw \\
a& & \qw    & \qw   & \qw    & \qw   & \qw & \qw &\qw &\qw &\qw & \qw \\
}
\end{array}$$
The simulation qubits are now representing a separate coin toss. To get more familiar with the notations, one mathematical way to write this circuit is
$$
H^1 H^2 H^3 H^4  \ket{0} \ket{0} \ket{0} \ket{0} \ket{a = 0}
$$
$$
=\frac{1}{\sqrt{2}}(\ket{0} + \ket{1})\frac{1}{\sqrt{2}}(\ket{0} + \ket{1})\frac{1}{\sqrt{2}}(\ket{0} + \ket{1})\frac{1}{\sqrt{2}}(\ket{0} + \ket{1}) \ket{a = 0}
$$
$$
= \frac{1}{4}\sum_{i=0}^{2^4-1} \ket{i} \ket{a=0},
$$
where the sum runs over all 16 four bit binary strings (0000, 0001, 0010, etc..).
In order to extract the probability of measuring four heads, we could use a multi controlled gate along with measurements:
$$\begin{array}{c}
\Qcircuit @C=2em @R=1em {
c_1& & \gate{H} & \ctrl{4}      & \qw      & \qw     & \qw  & \qw & \qw & \qw & \qw & \qw  \\
c_2& & \gate{H}      & \ctrl{3} & \qw      & \qw     & \qw  & \qw & \qw & \qw &\qw & \qw  \\
c_3& & \gate{H}      & \ctrl{2}      & \qw & \qw     & \qw  & \qw & \qw & \qw&\qw & \qw  \\
c_4& & \gate{H}      & \ctrl{1}      & \qw      & \qw & \qw  & \qw & \qw & \qw&\qw & \qw \\
a& & \qw    & \targ   & \qw    & \qw   & \qw & \qw &\qw &\qw &\qw & \meter \\
}
\end{array}$$
We can see that the ancilla qubit is now flipped to the $\ket{1}$ state if, and only if, all the simulation qubits are in the $\ket{1}$ state, because of the multi-controlled $X$ gate. This property is called entanglement, that is, the state of the ancilla cannot be described independently of the $\ket{c_1}\ket{c_2}\ket{c_3}\ket{c_4}$ state. The wanted probability is now extracted by repeatedly running this circuit and calculating $|\bra{1}\ket{a}|^2$, or in other words: Count the number of times $\ket{a}$ is in the $\ket{1}$ state and divide by the total number of experiments. Even though this circuit is simple, it captures most of what is needed to understand more advanced circuits. 

\subsubsection{Circuit depth}
\label{subsubsec:CircuitDepth}

For Noisy Intermediate-Scale Quantum Technology (NISQ), the ability to achieve sensible results relies on the execution time of the circuit for the problem at hand \cite{circuitdepth}. Circuit depth is an important quantity that is describing the number of time steps (time complexity) required to perform the circuit \cite{circuitdepth}.
As current quantum devices are not able to maintain a stable state for a long time \cite{decoherence}, achieving a short circuit depth is important if one expects to be able to run a quantum algorithm successfully. We will not go into specific details on how to calculate this quantity as it is rather simple to get the depth of a circuit when utilizing Qiskit \cite{qiskit}, the Python package we will use to write quantum algorithms. It is nevertheless important to keep the rough explanation of this quantity in mind as we will later talk about methods of reducing it.

\section{Quantum Phase Estimation}
\label{sec:QPE}
The first we will use to approximate the eigenvalues of the pairing Hamiltonian (eq. (\ref{eq:SimplifiedPairingHamiltonian})) is called the quantum phase estimation (QPE) algorithm. An important sub-routine of this algorithm is called the quantum Fourier transform. 
\subsection{Quantum Fourier Transform}
\label{subsec:QFT}

The quantum Fourier transform (QFT) is a linear transformation on qubits, which is used quite frequently in quantum algorithms. The QFT performed on an orthonormal basis yields the following state \cite{NielsenAndChuang}
\begin{equation}
    \label{eq:QFT}
    \ket{j} \rightarrow \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1}e^{2\pi i jk/ N}\ket{k}.
\end{equation}
How this transformation can become useful in different algorithms may not be apparent at first glance, but bear with me. We will quickly see its usefulness in the next section. In order to see how we can implement this transformation on a quantum computer, we first rewrite eq. (\ref{eq:QFT}) in a more convenient form. First we write the $n$-qubit state $\ket{j}$ using the binary representation
\begin{align}
    \label{eq:binaryRep}
    j &= j_1 j_2 \cdots j_n \notag \\
    j &= j_1 2^{n-1} + j_2 2^{n-2} + \cdots + j_n 2^0.
\end{align}
It is also useful to denote 
\begin{equation}
    \label{eq:binaryFracRep}
    0.j_1j_2 \cdots j_n = j_1/2 + j_2 /2^2 + \cdots + j_n / 2^n,
\end{equation}
as the binary fraction $0.j$.
Eq. (\ref{eq:QFT}) for an $n$-qubit state can be written as
\begin{align*}
    \frac{1}{2^{n/2}} \sum_{k=0}^{2^n - 1} e^{2\pi i jk / 2^n}\ket{k}.
\end{align*}
Utilizing the binary representation of k (eq. (\ref{eq:binaryRep})) gives us
\begin{align*}
    = \frac{1}{2^{n/2}} \sum_{k_1=0}^1 \cdots\sum_{k_n=0}^1 e^{2\pi ij (\sum_{l=1}^n k_l 2^{n-l})/2^n)}\ket{k_1\cdots k_n} \notag \\
    = \frac{1}{2^{n/2}} \sum_{k_1=0}^1 \cdots\sum_{k_n=0}^1 e^{2\pi ij (\sum_{l=1}^n k_l 2^{-l})}\ket{k_1\cdots k_n} \notag \\
    =\frac{1}{2^{n/2}} \sum_{k_1=0}^1 \cdots\sum_{k_n=0}^1 \bigotimes_{l=1}^n e^{2\pi i j k_l 2^{-l}}\ket{k_l} \notag \\
    = \frac{1}{2^{n/2}} \bigotimes_{l=1}^n \sum_{k_l = 0}^1 e^{2\pi i j k_l 2^{-l}}\ket{k_l}. \notag \\
\end{align*}
Next, we insert $k_l = 0$ and $k_l = 1$
\begin{align*}
     = \frac{1}{2^{n/2}} \bigotimes_{l=1}^n [\ket{0} + e^{2\pi i j 2^{-l}}\ket{1} ]. \notag \\
\end{align*}
We then use the binary representation of j (eq. (\ref{eq:binaryRep}))
\begin{align*}
    =\frac{1}{2^{n/2}} \bigotimes_{l=1}^n [\ket{0} + e^{2\pi i \sum_{i=1}^n j_i 2^{n-l-i}}\ket{1} ]. \notag \\
\end{align*}
Observe that when $n-l-i \geq 0$, we will just be multiplying the $\ket{1}$ state with 1. Therefore, we have
\begin{align}
    \label{eq:productRepresenation}
    = \frac{1}{2^{n/2}} \left( \ket{0} + e^{2\pi i 0.j_n}\ket{1} \right)\left(\ket{0} + e^{2\pi i 0.j_{n-1}j_n}\ket{1} \right)\cdots \left(\ket{0} + e^{2\pi i 0.j_1j_2\cdots j_n}\ket{1} \right),
\end{align}
which is the desired representation of the Fourier transform.
Let us see how to get from an arbitrary state $\ket{j_1j_2\cdots j_n}$ to the product representation in eq. (\ref{eq:productRepresenation}). We will skip any global normalization factors.
First we apply a Hadamard gate (eq. (\ref{eq:HadamardGate})) to the first qubit:
$$H^1 \ket{j_1 j_2 \cdots j_n} = (\ket{0} + e^{2\pi i 0.j_1}\ket{1})\ket{j_2\cdots j_n}. $$
Even though this is not the usual way to write a Hadamard transformed qubit, it is correct since $e^{2\pi i 0.j_1} = -1$ if $j_1$ = 1 and its 1 if $j_1 = 0$. For the next step, we need to apply a controlled $R_2$ gate to the first qubit, conditioned on the second qubit (see equation \ref{eq:Rkgate} for the $R_k$ gate, with $k = 2$). This action results in 
$$(\ket{0} + e^{2\pi i 0.j_1 + 2\pi i j_2/2^2}\ket{1})\ket{j_2\cdots j_n} = (\ket{0} + e^{2\pi i 0.j_1j_2}\ket{1})\ket{j_2\cdots j_n}. $$
Continuing with applying a controlled $R_k$ gate on the first qubit conditioned on qubit $l$ for $l=3,4,...,n$, we will end up with
$$ (\ket{0} + e^{2\pi i 0.j_1\cdots j_n}\ket{1})\ket{j_2\cdots j_n}.$$
We can now apply the Hadamard gate to the second qubit and utilize the controlled $R_k$ gate on the preceeding qubits in the same manner. Continuing this way until we reach the final qubit will put us in the desired state.
The complete circuit is illustrated below:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/methods/QFTCircuit.png}
    \caption{QFT circuit. Figure is taken from reference \cite{NielsenAndChuang}.}
    \label{fig:QFTCircuit}
\end{figure}
The gate requirement of the circuit in figure \ref{fig:QFTCircuit} is given by \cite{NielsenAndChuang}
\begin{equation}
    \label{eq:QFTcomplexity}
    \textbf{QFT Complexity: } \mathcal{O}(n^2),
\end{equation}
where $n$ is the number of qubits.
\subsection{Phase Estimation Algorithm}
\label{subsec:PhaseEstimation}

Now that we have shown how to do a Quantum Fourier transform, the next question one may ask is how to make use of it. A central procedure in many quantum algorithms is known as quantum phase estimation (QPE). Suppose we have a unitary operator $U$. This operator has an eigenvector $\ket{u}$, with the corresponding eigenvalue $e^{2 \pi i \lambda }$, where $\lambda$ is unknown. That is
\begin{equation}
    \label{eq:PhaseEstimationEigenvec1}
    U\ket{u} = e^{2\pi i \lambda} \ket{u}.
\end{equation}
The purpose of the QPE algorithm is to estimate $\lambda$. The first stage of the algorithm is shown in the figure below:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{figures/methods/PhaseEstimationCircuit.png}
    \caption{Phase estimation circuit. Figure is taken from reference \cite{NielsenAndChuang}.}
    \label{fig:QPECircuit}
\end{figure}

What we see in fig \ref{fig:QPECircuit} is that we have prepared two quantum registers. The first register has $t$ qubits, all initialised in the $\ket{0}$ state, and the second register is initialized as the eigenvector $\ket{u}$. We will refer to the register with $t$ qubits as the $t$-register, while we will refer to the second register as the $u$-register. All the qubits in the $t$-register are put in a superposition by applying the Hadamard gate (eq. (\ref{eq:HadamardGate})). We then apply the $U^{2^0}$ operator to the $u$-register, conditional on the bottom qubit in the $t$-register. This yields
$$\ket{u}\frac{1}{2^{t/2}} (\ket{0} + \ket{1})\cdots(\ket{0} + \ket{1}) \left( \ket{0} + e^{2\pi i 2^0 \lambda}\ket{1} \right),$$
which can be seen from eq. (\ref{eq:PhaseEstimationEigenvec1}).
Applying the $U^{2^1}$ operator to the $u$-register conditional on the next qubit in the $t$-register gives
$$\ket{u}\frac{1}{2^{t/2}}(\ket{0} + \ket{1})\cdots(\ket{0} + \ket{1})  \left( \ket{0} + e^{2\pi i 2^1 \lambda}\ket{1} \right)\left( \ket{0} + e^{2\pi i 2^0 \lambda}\ket{1} \right). $$
Continuing as shown in the circuit gives us finally
$$\ket{u}\frac{1}{2^{t/2}} \left( \ket{0} + e^{2\pi i 2^{t-1}  \lambda}\ket{1} \right)\cdots \left( \ket{0} + e^{2\pi i 2^2 \lambda}\ket{1} \right) \left( \ket{0} + e^{2\pi i 2^1 \lambda}\ket{1} \right)\left( \ket{0} + e^{2\pi i 2^0 \lambda}\ket{1} \right). $$
Now suppose that we can write the phase exactly as the binary fraction $$0.\lambda_1 \lambda_2 \cdots \lambda_t = \lambda_1/2 +  \lambda_2/2^2  + \cdots + \lambda_t /2^t.$$
The bottom qubit in the t-register can then be written as
$$ \ket{0} + e^{2\pi i 2^0 \lambda}\ket{1} = \ket{0} + e^{2\pi i (\lambda_1/2 +  \lambda_2/2^2  + \cdots + \lambda_t /2^t)}\ket{1} $$
$$= \ket{0} + e^{2\pi i 0.\lambda_1 \lambda_2 \cdots \lambda_t}\ket{1}. $$
For the next to bottom qubit in the $t$-register, we can write

$$ \ket{0} + e^{2\pi i 2^1 \lambda}\ket{1} = \ket{0} + e^{2\pi i 2(\lambda_1/2 +  \lambda_2/2^2  + \cdots + \lambda_t /2^t)}\ket{1} .$$
Since $\lambda_1$ has to be either 1 or 0 we can write this as
$$\ket{0} + e^{2\pi i 2(\lambda_1/2 +  \lambda_2/2^2  + \cdots + \lambda_t /2^t)}\ket{1} =\ket{0} + e^{2\pi i \lambda_1}e^{2\pi i (\lambda_2/2^1  + \cdots + \lambda_t /2^{t-1})}\ket{1} $$ 
$$= \ket{0} + e^{2\pi i0.\lambda_2\lambda_3\cdots\lambda_t}\ket{1} .$$
Doing this for all the qubits in the $t$-register gives us

\begin{equation}
    \label{eq:PhaseFirstStage}
    \frac{1}{2^{t/2}}  \left(  \ket{0} + e^{2\pi i 0.\lambda_t}  \right) \cdots\left(  \ket{0} + e^{2\pi i 0.\lambda_2\cdots\lambda_t}  \right)\left(  \ket{0} + e^{2\pi i 0.\lambda_1\cdots\lambda_t}  \right).
\end{equation}
Comparing this state with the QFT state in equation (eq. (\ref{eq:productRepresenation})), we see that we have the QFT of the state $\ket{\lambda}$. By performing the inverse quantum Fourier transform on the state in eq. (\ref{eq:PhaseFirstStage}), we will end up with

\begin{align}
    \label{eq:QPEFinalStep}
    (QFT)^{-1} \frac{1}{2^{t/2}}  \left(  \ket{0} + e^{2\pi i 0.\lambda_t}  \right) \cdots\left(  \ket{0} + e^{2\pi i 0.\lambda_2\cdots\lambda_t}  \right)\left(  \ket{0} + e^{2\pi i 0.\lambda_1\cdots\lambda_t}  \right)\ket{u} \notag \\ 
    = \ket{\lambda_1\lambda_2\cdots\lambda_t}\ket{u}.
\end{align}
In other words, we get the exact phase encoded in the $t$-register.
In reality though, we will not necessarily know the eigenvector $\ket{u}$. To deal with this, we can prepare the $u$-register in a state $\ket{\psi} = \sum_{i=1}^{n} c_i \ket{u_i}$ which is a linear combination of the eigenstates of $U$. Repeated applications of the phase estimation algorithm followed by measurements will then yield a specter of eigenvalues and eigenvectors. We also can not always express the phase exactly as a $t$-bit binary fraction. It can be shown that we will with high probability produce a pretty good estimation to $\lambda$ nevertheless \cite{NielsenAndChuang}.

Since all operations on qubits are unitary, we can complex conjugate all the operations in the QFT circuit (fig. \ref{fig:QFTCircuit}) and apply them in the reverse order to yield the inverse Fourier transform. We can show this by considering some arbitrary unitary operations on a state $\ket{j}$:
$$ABCD\cdots N \ket{j} = \ket{k}$$
$$(ABCD\cdots N )^{\dagger} \ket{k} = (ABCD\cdots N )^{\dagger}ABCD\cdots N \ket{j}$$
$$= N^\dagger \cdots D^\dagger C^\dagger B^\dagger A^\dagger ABCD\cdots N \ket{j} = \ket{j}.$$

\bigskip
\noindent
To summarize, the QPE algorithm is performed by first applying the circuit in figure \ref{fig:QPECircuit} to the $u$ and $t$-register. We then apply the inverse QFT (section \ref{subsec:QFT}) on the $t$-register. The inverse QFT requires $\mathcal{O}(t^2)$ operations (eq. (\ref{eq:QFTcomplexity})), where $t$ is the number of qubits in the $t$-register. The complexity of the complete QPE algorithm is then dependent on the number of operations required to implement the controlled $U^{2^j}$ operations.

Our plan is to use this method to approximate the eigenvalues of the pairing Hamiltonian (eq. (\ref{eq:SimplifiedPairingHamiltonian})). Before we do this, we will have to introduce two important formulas.

\subsection{The Suzuki-Trotter transformation}
\label{subsec:SuzukiTrotter}
The Suzuki-Trotter approximation states that given some unitary operators $\hat{A}_1, \hat{A}_2, \hat{A}_3,\cdots$ that do not necessarily commute, we have for any real $t$
\begin{equation}
    \label{eq:SuzukiTrotterApprox}
    e^{it(\hat{A}_1 + \hat{A}_2 + \hat{A}_3 + \cdots)} = \lim_{m\rightarrow \infty} (e^{i\frac{t}{m}\hat{A}_1}e^{i\frac{t}{m}\hat{A}_2}e^{i\frac{t}{m}\hat{A}_3}\cdots)^m.
\end{equation}
This can be shown by utilizing the Taylor expansion of $e^{it(\hat{A}_1 + \hat{A}_2 + \hat{A}_3 + \cdots)}$. We can also in the same manner show that \cite{NielsenAndChuang}
\begin{equation}
    \label{eq:SuzukiTrotterApproxWithFault}
    e^{i\Delta t(\hat{A}_1 + \hat{A}_2 + \hat{A}_3 + \cdots)} =  e^{i\Delta t\hat{A}_1}e^{i\Delta t\hat{A}_2}e^{i\Delta t \hat{A}_3}\cdots + \mathcal{O}(\Delta t^2 ),
\end{equation}
hence we can approximate the action of $e^{it(\hat{A}_1 + \hat{A}_2 + \hat{A}_3 + \cdots)}$ to arbitrary precision by utilizing eq. (\ref{eq:SuzukiTrotterApproxWithFault}) repeatedly with small enough $\Delta t$.

\subsection{The Jordan-Wigner transformation}
\label{subsec:JordanWignerTransformation}
The Jordan-Wigner transformation is a transformation that maps the Pauli gates (eq. (\ref{eq:PauliMatrices})) onto fermionic creation and annihilation operators \cite{Nielsen2005TheFC}. The creation and annihilation operators from the second quantization formalism (see section \ref{sec:SecondQuantization}) can then be represented on quantum computers, and we will be able to rewrite our second quantization Hamiltonian (eq. (\ref{eq:SimplifiedPairingHamiltonian})) in terms of quantum gates. Suppose that we represent a qubit in state $\ket{0}$ as a state occupied with a fermion and $\ket{1}$ as a state with no fermion. We then see that the operators
\begin{align}
    \label{eq:sigmaplussigmaminus}
    \sigma_+ &= \frac{1}{2}(\sigma_x + i\sigma_y) = \begin{bmatrix}
    0 & 1  \\
    0 & 0
\end{bmatrix} \notag \\
    \sigma_- &= \frac{1}{2}(\sigma_x - i\sigma_y) = \begin{bmatrix}
    0 & 0  \\
    1 & 0
\end{bmatrix},
\end{align}
have the following effect on the qubit basis states
$$\sigma_+ \ket{1} = \ket{0} \qquad \sigma_- \ket{0} = \ket{1},$$
and
$$\sigma_+ \ket{0} = 0 \qquad \sigma_-\ket{1} = 0.$$
Hence, $\sigma_+$ acts as a creation operator and $\sigma_-$ acts as an annihilation operator. However, since fermionic states are anti-symmetric, $a^\dagger_a a^\dagger_b \ket{c} = - a^\dagger_b a^\dagger_a \ket{c}$, we need our quantum gate representation of the creation/annihilation operators to preserve this property. This can be achieved by multiplying the $\sigma_z$ matrix (eq. (\ref{eq:PauliMatrices})) on all the occupied states leading up to the one we operate on. The complete creation and annihilation operators can then be represented as
\begin{equation}
    \label{eq:LadderOpsPauli}
    a^\dagger_n \equiv \left(\prod_{k=1}^{n-1}\sigma_z^k \right)\sigma_+^n \qquad a_n \equiv \left(\prod_{k=1}^{n-1}\sigma_z^k \right) \sigma_-^n
\end{equation}
where the superscript tells us which qubit the operator acts on. For convenience, we chose that odd qubits are in a spin up state, while even qubits are in spin down state. For example for the following state
\begin{align*}
    \textit{Qubit state:} & \ket{0 \: \; \ 0 \: \; \ 1 \: \; \ 1 } \\
    \textit{Spin state:} & + \; - \; + \; - \; \\
    \textit{Spacial state:} & \ 1 \; \: \ 1 \ \; \: 2 \ \; \: 2 \; ,
\end{align*}
the first spacial basis state is occupied with a fermion pair with opposite spin, while the second spacial state is not occupied with any fermions.
The equivalent of the reference state in eq. (\ref{eq:ReferenceState}) for our qubit-state is then
\begin{equation}
    \label{eq:qubitReferenceState}
    \ket{\Psi_0} = \ket{0}^{\otimes^n}\ket{1}^{\otimes^k},
\end{equation}
where $n$ is the number of particles and $n+k$ is the number of spin-orbitals.

\subsubsection{Jordan-Wigner transformation of Pairing Hamiltonian}
\label{subsubsec:JordanWignerPairing}
We can now write the second quantization pairing Hamiltonian (eq. (\ref{eq:SimplifiedPairingHamiltonian})) in terms of the Pauli matrices. A detailed derivation is provided in appendix B. Here we simply state the result: For the one body part of the Hamiltonian we have the terms
\begin{equation}
    \label{eq:Onebodyintermsofpauli}
    \hat{H}_{0p} = \frac{1}{2}\delta(p - 1 - I[p\%2=0])(I^p + \sigma_z^p),
\end{equation}
where we sum over each qubit $p$ and $\%$ is the modulo operator. We have that $I[f(x) = y] = 1$ if $f(x) = y$ and zero otherwise. For the interaction term we have two possibilities. First for $p = q$ we have:
\begin{align}
    \label{eq:twobodypaulipisq}
    \hat{V}_{p} = -\frac{1}{8}g\Big[ &I^{\otimes^{2p-2}} \otimes I  \otimes I \otimes I^{\otimes^{n - 2p}} \nonumber \\
    +& I^{\otimes^{2p-2}} \otimes I  \otimes \sigma_z \otimes I^{\otimes^{n - 2p}} \\
    +& I^{\otimes^{2p-2}} \otimes \sigma_z  \otimes I \otimes I^{\otimes^{n - 2p}} \nonumber \\
    +& I^{\otimes^{2p-2}} \otimes \sigma_z  \otimes \sigma_z \otimes I^{\otimes^{n - 2p}}\Big],\nonumber 
\end{align}
and for $q - p \geq 1$ we get:
\begin{align}
    \label{eq:twobodypauliqgeqp}
       \hat{V}_{pq} = -\frac{1}{16}g [& I^{\otimes^{2p -2}} \otimes \sigma_x \otimes \sigma_x \otimes I^{\otimes^{2(q - p - 1)}} \otimes \sigma_x \otimes \sigma_x \otimes I^{\otimes^{n-2q}} \nonumber \\
        & -I^{\otimes^{2p -2}} \otimes \sigma_x \otimes \sigma_x \otimes I^{\otimes^{2(q - p - 1)}} \otimes \sigma_y \otimes \sigma_y \otimes I^{\otimes^{n-2q}} \nonumber \\
        & + I^{\otimes^{2p -2}} \otimes \sigma_x \otimes \sigma_y \otimes I^{\otimes^{2(q - p - 1)}} \otimes \sigma_x \otimes \sigma_y \otimes I^{\otimes^{n-2q}} \nonumber \\
        & + I^{\otimes^{2p -2}} \otimes \sigma_x \otimes \sigma_y \otimes I^{\otimes^{2(q - p - 1)}} \otimes \sigma_y \otimes \sigma_x \otimes I^{\otimes^{n-2q}}  \\
        &+ I^{\otimes^{2p -2}} \otimes \sigma_y \otimes \sigma_x \otimes I^{\otimes^{2(q - p - 1)}} \otimes \sigma_x \otimes \sigma_y \otimes I^{\otimes^{n-2q}} \nonumber \\
        & + I^{\otimes^{2p -2}} \otimes \sigma_y \otimes \sigma_x \otimes I^{\otimes^{2(q - p - 1)}} \otimes \sigma_y \otimes \sigma_x \otimes I^{\otimes^{n-2q}} \nonumber \\
        & - I^{\otimes^{2p -2}} \otimes \sigma_y \otimes \sigma_y \otimes I^{\otimes^{2(q - p - 1)}} \otimes \sigma_x \otimes \sigma_x \otimes I^{\otimes^{n-2q}} \nonumber \\
        &+ I^{\otimes^{2p -2}} \otimes \sigma_y \otimes \sigma_y \otimes I^{\otimes^{2(q - p - 1)}} \otimes \sigma_y \otimes \sigma_y \otimes I^{\otimes^{n-2q}}].\nonumber 
\end{align}
We have included a factor of two so the sum over $p$ and $q$ here can be restricted to $q > p$.
The complete Jordan-Wigner transformed pairing Hamiltonian can then be written as
\begin{equation}
    \label{eq:JordanWignerHamiltonian}
    \hat{H} = \sum_p \hat{H}_{0p} + \sum_{p} \hat{V}_p + \sum_{q > p} \hat{V}_{pq},
\end{equation}
where $\hat{H}_{0p}$, $\hat{V}_p$ and $\hat{V}_{pq}$ is given by eqs. (\ref{eq:Onebodyintermsofpauli}), (\ref{eq:twobodypaulipisq}) and (\ref{eq:twobodypauliqgeqp}), respectively.


\subsection{Hamiltonian Simulation}
\label{subsec:HamiltonianSimulation}
We are now ready to find the eigenvalues of a fermionic Hamiltonian using the quantum phase estimation algorithm (section \ref{subsec:PhaseEstimation}). To see how this can be done, consider that a (time independent) quantum state evolves according to the time evolution operator
\begin{equation}
    \label{eq:TimeEvolution}
    \ket{\psi (t)} = e^{-i\hat{H} t/\hbar}\ket{\psi (0)} = \hat{U} \ket{\psi (0)},
\end{equation}
where $\hat{H}$ is the Hamiltonian.
We also know that an eigenstate $\psi_k$ of the $\hat{H}$ is also an eigenstate of the time evolution operator. Its eigenvalue is given by
\begin{equation}
    \label{eq:TimeEvoEigenval}
    e^{-i\hat{H} t/\hbar}\ket{\psi_k} = e^{-i E_k t / \hbar }\ket{\psi_k},
\end{equation}
where $E_k$ is the $k$'th eigenvalue of $\hat{H}$.
The QPE algorithm finds the phase $\lambda_k$ of a unitary operator with eigenvalue $e^{-i\lambda_k 2\pi}$, so if we can approximate the time evolution operator on a quantum computer, we can also approximate its eigenvalues with the QPE algorithm. 
We have shown in section \ref{subsubsec:JordanWignerPairing} how to write our Hamiltonian in terms of the Pauli matrices:
\begin{equation}
    \label{eq:FullPauliHamiltonian}
    \hat{H} = \sum_p \hat{H}_{0p} + \sum_{p} \hat{V}_p + \sum_{q > p} \hat{V}_{pq},
\end{equation}
where $\hat{H}_{0p}$, $\hat{V}_p$ and $\hat{V}_{pq}$ is given by eqs. (\ref{eq:Onebodyintermsofpauli}), (\ref{eq:twobodypaulipisq}) and (\ref{eq:twobodypauliqgeqp}), respectively.
The corresponding time evolution operator is then given by
\begin{equation}
    \label{eq:TimeEvoPauli}
    \hat{U}(t) = e^{-i(\sum_p \hat{H}_{0p} + \sum_{p} \hat{V}_p + \sum_{q > p} \hat{V}_{pq})t}.
\end{equation}
The Suzuki-Trotter approximation in eq. (\ref{eq:SuzukiTrotterApproxWithFault}) can now be utilized to approximate a small time-step with this operator:
\begin{equation}
    \label{eq:TimeEvoTrotterApprox}
    \hat{U}(\Delta t) = \prod_p e^{-i\hat{H}_{0p}\Delta t}\prod_p e^{-i\hat{V}_p \Delta t} \prod_{q>p} e^{-i\hat{V}_{pq} \Delta t} + \mathcal{O}(\Delta t^2).
\end{equation}
We will now show how each of the separate exponential operators in eq. (\ref{eq:TimeEvoTrotterApprox}) can be implemented on a quantum computer. Consider that $e^{-i \sigma_z \otimes \sigma_z \otimes \sigma_z \otimes \sigma_z \Delta t}$ can be implemented with the following circuit \cite{NielsenAndChuang}
\begin{equation}
   \label{circuit:TimeEvolution}
   e^{-i \sigma_z \otimes \sigma_z \otimes \sigma_z \otimes \sigma_z \Delta t} \equiv
    \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \ctrl{4} & \qw      & \qw      & \qw     & \qw  & \qw & \qw & \qw & \ctrl{4} & \qw  \\
& \qw      & \ctrl{3} & \qw      & \qw     & \qw  & \qw & \qw & \ctrl{3} &\qw & \qw  \\
& \qw      & \qw      & \ctrl{2} & \qw     & \qw  & \qw & \ctrl{2} & \qw&\qw & \qw  \\
& \qw      & \qw      & \qw      & \ctrl{1}& \qw  & \ctrl{1} & \qw & \qw&\qw & \qw \\
\lstick{\ket{0}}& \targ    & \targ    & \targ    & \targ   & \gate{e^{-i\Delta t Z}} & \targ &\targ &\targ &\targ & \qw \\
}
\end{array},
\end{equation}
where the operator $e^{-i\Delta t Z}$ is given by the $R_z(\theta)$ gate (eq. (\ref{eq:RotationOps})). We can easily extend this circuit with more (less) control qubits to apply longer (shorter) strings of $\sigma_z$ gates.

With the use of the following identities
\begin{equation}
    \label{eq:XintermsofZ}
    \sigma_x = H\sigma_zH,
\end{equation}
and
\begin{equation}
    \label{eq:YintermsofZ}
    \sigma_y = R_z(\pi/2)H\sigma_z H R_z(-\pi/2),
\end{equation}
we can use circuit (\ref{circuit:TimeEvolution}) to implement the time evolution of an arbitrary string of Pauli matrices. For example, for the tensor product $H = \sigma_x \otimes \sigma_y \otimes \sigma_x \otimes \sigma_y$, we have
\begin{align}
    \label{eq:TwobodyHamilExample}
    &H \otimes R_z(\pi/2)H \otimes H \otimes R_z(\pi/2)H \notag \\
    \times  & e^{-i\Delta t \sigma_z\otimes \sigma_z \otimes \sigma_z \otimes \sigma_z}  \notag \\
    \times & H \otimes HR_z(-\pi/2) \otimes H \otimes HR_z(-\pi/2) \notag \\
    = & U e^{-i\Delta t \sigma_z\otimes \sigma_z \otimes \sigma_z \otimes \sigma_z} U^\dagger \notag \\
    =& U [cos(\Delta t)I - isin(\Delta t) \sigma_z\otimes \sigma_z \otimes \sigma_z \otimes \sigma_z]U^\dagger \notag \\
    =& cos(\Delta t)I - isin(\Delta t) \sigma_x \otimes \sigma_y \otimes \sigma_x \otimes \sigma_y \notag \\
    =& e^{-i\Delta t \sigma_x \otimes \sigma_y \otimes \sigma_x \otimes \sigma_y}, \notag
\end{align}
since $U U^\dagger = I$ and
$$ U \times (\sigma_z\otimes \sigma_z \otimes \sigma_z \otimes \sigma_z) \times U^\dagger $$
$$=(H \otimes R_z(\pi/2)H \otimes H \otimes R_z(\pi/2)H)$$
$$\times (\sigma_z\otimes \sigma_z \otimes \sigma_z \otimes \sigma_z)$$ $$\times (H \otimes HR_z(-\pi/2) \otimes H \otimes HR_z(-\pi/2))$$
$$= H\sigma_z H \otimes R_z(\pi/2)H\sigma_z H R_z(-\pi/2) \otimes H \sigma_z H \otimes R_z(\pi/2) H \sigma_z H R_z(-\pi/2)$$
$$ = \sigma_x \otimes \sigma_y \otimes \sigma_x \otimes \sigma_y. $$

Hence, if we have a Pauli operator $\sigma_a$, where $a \in {x,y,z}$ and $\sigma_a = U_a \sigma_z U_a^\dagger $, we can implement the time evolution $e^{-\Delta t h \sigma_a \otimes \sigma_b \otimes \sigma_c \otimes \sigma_d}$ with the following circuit
\begin{equation}
   \label{circuit:TimeEvolutionArbitraryPauli}
    \begin{array}{c}
\Qcircuit @C=1.5em @R=1em {
& \gate{U_a} & \ctrl{4} & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \ctrl{4} & \gate{U_a^\dagger} \\
& \gate{U_b} & \qw  & \ctrl{3} & \qw & \qw & \qw & \qw & \qw & \ctrl{3} & \qw & \gate{U_b^\dagger} \\
& \gate{U_c} & \qw & \qw & \ctrl{2} &\qw & \qw & \qw & \ctrl{2} & \qw & \qw &\gate{U_c^\dagger} \\
& \gate{U_d} & \qw  & \qw & \qw & \ctrl{1} &\qw & \ctrl{1} & \qw & \qw & \qw & \gate{U_d^\dagger} \\
& \qw & \targ & \targ & \targ &  \targ & \gate{R_z(2\Delta t h)} & \targ & \targ & \targ & \targ & \qw
}
\end{array},
\end{equation}
where $h$ is a real factor. We can see that this circuit is efficient since we at most require an amount of operations linear in the amount of qubits. Hence, we can simulate the time evolution of any Hamiltonian efficiently as long as the number of terms in the Hamiltonian is polynomial in the amount of qubits \cite{NielsenAndChuang}. We see from the Jordan-Wigner transformation of our pairing Hamiltonian (section \ref{subsec:JordanWignerTransformation}) that this is the case for our problem.


\subsection{Getting the complete eigenvalue spectra}
\label{subsubsec:EigenvalueSpectraQPE}

The phase estimation algorithm is aimed at finding $\lambda_k$ for a unitary operator $\hat{U}$ with eigenvalue $e^{i2\pi \lambda_k}$, such that
\begin{equation*}
    \hat{U}\ket{\psi_k} = e^{i2\pi\lambda_k}\ket{\psi_k}.
\end{equation*}
In our case, the time evolution operator applied for a time $\tau$ has the eigenvalues $e^{-iE_k \tau}$, which means that the value we read from the phase estimation algorithm is 
$$ i2\pi \lambda_k = - iE_k \tau $$
$$ \implies \lambda_k = -\frac{iE_k\tau}{i2\pi} $$
\begin{equation}
    \label{eq:PhaseEstimationLambdaInTermsOfEigenvalue}
    \implies \lambda_k = -E_k \tau /2\pi.
\end{equation}
An assumption with the phase estimation algorithm is that we can write the eigenvalue as a binary fraction. Since a binary fraction is a positive number, we need our eigenvalues to be negative for the phase estimation algorithm to work, according to the above equation for $\lambda_k$ (eq. (\ref{eq:PhaseEstimationLambdaInTermsOfEigenvalue})). We can force this by subtracting a large enough constant value $E_{max}$ from the Hamiltonian, since $$(\hat{H} - E_{max})\ket{\phi_k} = (E_k - E_{max})\ket{\phi_k}.$$
This gives us
$$\lambda_k = -(E_k - E_{max})\tau /2\pi $$
$$\implies \lambda_k 2\pi / \tau = E_{max} - E_k $$
\begin{equation}
    \label{eq:PhaseEstimationMeasurementToEigenvalue}
   \implies E_k = E_{max} - \lambda_k 2\pi/\tau.
\end{equation}
Further, since the phase estimation algorithm yields the following state for the $t$-register (eq. (\ref{eq:QPEFinalStep}))
$$\ket{\lambda_1 \lambda_2 \cdots \lambda_{n_t}} =\ket{\lambda 2^{n_t}},$$
where $n_t$ is the number of qubits in the $t$-register,
we need to transform the measured binary number $\lambda_1 \lambda_2 \cdots \lambda_{n_t}$ to a binary fraction $0.\lambda_1 \lambda_2 \cdots \lambda_{n_t}$ before plugging it into the equation for $E_k$.

We can also see that if we have $\lambda = \lambda^{'} + n > 1$ where $0 < \lambda^{'} < 1$ and $n$ is a positive integer, we get from the phase estimation algorithm 
$$e^{i2\pi (\lambda^{'} + n)} = e^{i2\pi n}e^{i2\pi \lambda^{'}} =e^{i2\pi \lambda^{'}}, $$
or written in binary form
$$e^{i2\pi (\lambda^{'} + n)} = e^{i2\pi \lambda_1\cdots\lambda_k.\lambda_{k+1}\cdots\lambda_n} = e^{i 2\pi 0.\lambda_{k+1}\cdots \lambda_n}.$$
In other words; for eigenvalues greater than one, we lose information.
A restriction on $\lambda_k < 1$ in eq. (\ref{eq:PhaseEstimationLambdaInTermsOfEigenvalue}) gives
$$-E_k \tau / 2\pi < 1 $$ 
$$\implies -E_k \tau < 2\pi $$
or
$$-(E_k - E_{max}) \tau < 2\pi. $$
Substituting $E_k$ with $E_{min}$ (the lowest eigenvalue of $\hat{H}$) gives an upper bound on $t$ in order to yield the whole eigenvalue spectrum
\begin{equation}
    \label{eq:PhaseEstimationtUpperBound}
    t < \frac{2\pi}{E_{max} - E_{min}}.
\end{equation}
We also have to keep in mind the number of qubits to use in the $t$-register. If we use $k$ qubits we can represent $2^k$ binary fractions. A quantum state represented by $s$ simulation qubits potentially has $2^s$ eigenvalues. With a $t$-register of $k$ qubits, this means that we will have $\frac{2^k}{2^s} = 2^{k-s}$ points for each eigenvalue.
Previous research has claimed that a surplus of around 5 qubits in the $t$-register are usually sufficient to yield the complete eigenvalue-spectra \cite{Ovrum2003QuantumCA}.

\subsection{Summary of the quantum phase estimation algorithm}
A summary of the QPE algorithm is explained below:
\begin{itemize}
    \item Subtract a constant $E_{max}$ from the problem Hamiltonian (see section \ref{subsubsec:EigenvalueSpectraQPE}). The constant should be larger than the largest eigenvalue of the Hamiltonian.
    \item Prepare two registers. One register of $t$-qubits ($t$-register) and one register of $u$ qubits ($u$-register) (see QPE circuit in figure \ref{fig:QPECircuit}).
    \item Put the $u$-register in a linear combinations of the eigenstates of the problem Hamiltonian $\hat{H}$. This can be done by applying a Hadamard gate (eq. (\ref{eq:HadamardGate})) to each of the qubits. This will yield a superposition of all the computational basis states.
    \item Apply the QPE circuit (figure \ref{fig:QPECircuit}), where $U$ is given by the Suzuki-Trotter approximation (section \ref{subsec:SuzukiTrotter}) of the Hamiltonian time evoulution operator. The evolution time is bounded by eq. (\ref{eq:PhaseEstimationtUpperBound}).
    \item Apply the inverse of the QFT ciruit (figure \ref{fig:QFTCircuit}) to the $t$-register.
    \item Measure all qubits in the $t$-register to yield a binary fraction
    \item Use eq. (\ref{eq:PhaseEstimationMeasurementToEigenvalue}) to obtain the measured eigenvalue from the binary fraction.
    \item Repeat to obtain a spectra of eigenvalues.
\end{itemize}


\section{Variational Quantum Eigensolvers}
\label{sec:VQE}

The variational principle states that the expectation value of the Hamiltonian has to be larger than or equal to the ground state energy of the system. Mathematically this can be expressed as
\begin{equation}
    \label{eq:VariationalPrinciple}
    \bra{\psi} H \ket{\psi} \geq E_0.
\end{equation}
We can understand this principle intuitively by considering that no single measurement of the energy can be lower than the ground state energy. Hence, the expectation value of the energy can neither. Variational methods make use of this principle by calculating the expectation value in equation  \ref{eq:VariationalPrinciple} for what we call a trial wavefunction $\ket{\psi_T(\boldsymbol{\theta)}}$:
$$\bra{\psi_T(\boldsymbol{\theta})} H \ket{\psi_T(\boldsymbol{\theta})} = E(\boldsymbol{\theta}).$$
The variational parameters $\boldsymbol{\theta} = [\theta_1, \theta_2, \cdots, \theta_p]$ are then varied to minimize $E(\boldsymbol{\theta})$, which hopefully makes a good approximation for $E_0$. For variational quantum eigensolvers (VQE), the trial wave function is given by a parametrized n-qubit state
$$ U(\boldsymbol{\theta}) \ket{\psi_0} = \ket{\psi_T(\boldsymbol{\theta})}, $$
where $U(\boldsymbol{\theta})$ is some parametrized multi-qubit gate and $\ket{\psi_0}$ is the initial state of the qubits.
As long as the Hamiltonian can be rewritten as a sum of quantum gates $O_i$
$$ H = \sum_{i=1}^m h_i O_i, $$
we can find its expectation value by considering the expectation of each term
\begin{equation}
    \label{eq:VQEExpectationValueofHamiltonian}
    \bra{\psi_T(\boldsymbol{\theta})} H \ket{\psi_T(\boldsymbol{\theta})} = \sum_{i=1}^m h_i \bra{\psi_T(\boldsymbol{\theta})} O_i \ket{\psi_T(\boldsymbol{\theta})}.
\end{equation}
How to perform these steps are best described by considering an example, namely the Max-Cut problem.

\subsection{Max-Cut problem}
\label{subsec:VQEMaxCut}
The Max-Cut problem is one of the hardest combinatorial optimization problems to solve, yet its one of the easiest to conceptualize. The aim of this section is to explain a quantum variational eigensolver by solving the Max-Cut problem. The Max-Cut problem can be understood by considering this graph
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/methods/maxcut.png}
    \caption{Unsolved Max-Cut graph}
    \label{fig:maxcutunsolved}
\end{figure}
The circles are called the nodes of the graph and the lines connecting two nodes are called edges. Now consider that you are allowed to color each node in either red or blue. The numbers next to the edges of the graph are called weights, and they state the number of points gained if the nodes the edge connects to are of different colors. Solving a Max-Cut problem corresponds to coloring the graph in such a way that you have the maximum amount of points.
A solution to this Max-Cut problem is represented in the graph below
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{figures/methods/maxcutsolved.png}
    \caption{Solved Max-Cut graph}
    \label{fig:maxcutsolved}
\end{figure}
yielding a total of 13 points. We gain points from all the weights except of the one representing the connection from node A to E, as these are both of the same coloring. Given a graph with $n$ nodes, all its information can be expressed with an $n$ by $n$ matrix $W$. Its entries $w_{ij}$ correspond to the points given by the edge connecting node $i$ and node $j$.
To give an example, the matrix for the graph given in figs. \ref{fig:maxcutunsolved} and \ref{fig:maxcutsolved} is

\[ W =
\begin{blockarray}{cccccc}
A & B & C & D & E \\
\begin{block}{(ccccc)c}
  0 & 3 & 0 & 0 & 1 & A \\
  3 & 0 & 2 & 0 & 3 & B \\
  0 & 2 & 0 & 2 & 0 & C \\
  0 & 0 & 2 & 0 & 3 & D \\
  1 & 3 & 0 & 3 & 0 & E
\end{block}
\end{blockarray}.
 \]
We can express a profit-function for the Max-Cut problem with the help of the matrix elements in any Max-Cut matrix $W$, by representing the color of node $i$ with a binary number $x_i \in \{0,1\}$ \cite{MaxCutAndEulerRotationHardwareEfficient}:
\begin{equation}
    \label{eq:MaxCutCostFunction}
    C(\boldsymbol{x};W) = \sum_{i,j} w_{ij}x_i(1 - x_j).
\end{equation}
The coloring $\boldsymbol{x}$ which yields this functions highest value is the solution to the Max-Cut problem. We now want to map the function $C(\boldsymbol{x})$ into a Hamiltonian in such a way that it can be evaluated on a quantum computer. This can be done by first expressing the coloring of a given $n$-noded graph with an $n$-qubit state $\ket{q_1}\ket{q_2}\cdots \ket{q_n}$, where $q_i$ corresponds to the coloring $x_i$. We then do the following mapping in the profit-function (eq. (\ref{eq:MaxCutCostFunction})) \cite{MaxCutAndEulerRotationHardwareEfficient}
\begin{equation}
    \label{eq:BinaryXtoPauli}
    x_i \rightarrow \frac{1 - \sigma_z^i}{2},
\end{equation}
where $\sigma_z$ is the Pauli-$Z$ gate (eq. (\ref{eq:PauliMatrices})).
We can see that this will successfully evaluate $C(\boldsymbol{x})$ since 
$$\frac{1 - \sigma_z^i}{2} \ket{0} = 0 \qquad \text{and} \qquad \frac{1 - \sigma_z^i}{2}\ket{1} = \ket{1}. $$
Inserting eq. (\ref{eq:BinaryXtoPauli}) into the cost function in eq. (\ref{eq:MaxCutCostFunction}) gives
$$ \sum_{i,j} w_{ij}x_i(1 - x_j) \rightarrow \sum_{i,j}w_{ij}\frac{1 - \sigma_z^i}{2}(1 - \frac{1 - \sigma_z^j}{2}) $$
$$=   \sum_{i,j}w_{ij}[\frac{1 - \sigma_z^i}{2} - \frac{1 - \sigma_z^i}{2}\frac{1 - \sigma_z^j}{2}]$$
$$= \sum_{i,j}w_{ij}\left( \frac{1 - \sigma_z^i}{2} - \frac{1}{4}[1 - \sigma_z^i - \sigma_z^j + \sigma_z^i\sigma_z^j]\right). $$
Since the terms without any Pauli matrices are constant when varying the quantum state, we can omit these terms from the above equation. Also, when dealing with terms with only a single qubit gate, we are free to exchange the variable $i$ with $j$ without altering the resulting equation. This removes all single-qubit gates. We are then free to multiply the equation with a factor of two to restrict the sum to $i < j$. When realizing that global factors do not contribute to the location of the minima in parameter space, we are left with \cite{MaxCutAndEulerRotationHardwareEfficient}
\begin{equation}
    \label{eq:maxcuthamiltonian}
    H = \sum_{i<j}w_{ij}\sigma_z^i \sigma_z^j.
\end{equation}
Now that we have the Hamiltonian for our Max-Cut problem, we need to explain how we exaluate the expectation values in eq. (\ref{eq:VQEExpectationValueofHamiltonian}).


\subsection{VQE Expectation Values}
\label{subsec:VQEExpecVals}

As stated earlier, the expectation value of our Hamiltonian is the sum of the expectation value of each term (see eq. (\ref{eq:VQEExpectationValueofHamiltonian})). We have set up an Hamiltonian for our problem, so how do we evaluate the expectation values then? Let us first consider how to handle Pauli-$Z$ expectation values, as the Max-Cut Hamiltonian (eq. (\ref{eq:maxcuthamiltonian})) is containing only Pauli-$Z$ gates. The first eigenstate of the Pauli-$Z$ matrix is $\ket{0}$ with an eigenvalue of 1 and the second is $\ket{1}$ with an eigenvalue of -1. We know from quantum mechanics that if we act upon a qubit with the Pauli-$Z$ gate and perform a measurement, it will collapse to one of its eigenstates. Therefore, if we measure the state $\sigma_z^i \sigma_z^j \ket{\psi_T( \boldsymbol{\theta})}$, we can retrieve the resulting eigenvalue by considering the state of qubit $i$ and $j$:
\begin{table}[H]
\centering
\label{tab:paulizEigenvaluesEigenstates}
\begin{tabular}{ll}
\quad $\ket{q_i} \ket{q_j} $             & Eigenvalue            \\
\quad $\ket{0} \ket{0} $ & $1 \cdot 1 = 1$       \\
\quad $\ket{1} \ket{1} $ & $(-1) \cdot (-1) = 1$ \\
\quad $\ket{0} \ket{1} $ & $1 \cdot (-1) = -1 $  \\
\quad $\ket{1} \ket{0}$  & $(-1) \cdot 1 = -1 $ 
\end{tabular}
\end{table}
The expectation value $\bra{\psi} \sigma_z^i \sigma_z^j \ket{\psi} $ is then approximated by repeatedly measuring $\sigma_z^i \sigma_z^j \ket{\psi}$ and averaging the obtained eigenvalues. The final step is to multiply the expectation value with the corresponding matrix element $w_{ij}$ (eq. (\ref{eq:maxcuthamiltonian})). This is done separately with each term in eq. (\ref{eq:maxcuthamiltonian}) to finally yield
$$ \bra{\psi_T(\boldsymbol{\theta})} H \ket{\psi_T(\boldsymbol{\theta})} = \sum_{i < j} w_{ij} \bra{\psi_T(\boldsymbol{\theta})} \sigma_z^i \sigma_z^j \ket{\psi_T(\boldsymbol{\theta})}.$$
The following circuit is to be run for all $i$ and $j$ subject to $i < j$ to obtain the above expectation values
\begin{equation}
   \label{circuit:MaxCutExpectationValueZ}
    \begin{array}{c}
\Qcircuit @C=2em @R=1em {
&& \qw & \qw & \qw & \qw \\
&& \vdots& \vdots & \vdots & \vdots \\
\ket{i}& & \gate{Z} & \qw & \qw & \meter \\
&& \vdots& \vdots & \vdots & \vdots \\
\ket{j} && \gate{Z} & \qw & \qw & \meter \\
&& \vdots& \vdots & \vdots & \vdots
}
\end{array}
\end{equation}

\bigskip

Here $\ket{i}$ denotes the $i$'th qubit.
From this example, we have learned that the circuit for finding the expectation for a Pauli-$Z$ matrix is
\begin{equation}
   \label{circuit:paulizExpectationValue}
    \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{Z} &  \meter 
}
\end{array}
\end{equation}
We will sometimes run into other matrices than the Pauli-$Z$ matrix, namely the Pauli-$X$ and Pauli-$Y$ matrices (eq. (\ref{eq:PauliMatrices})). To calculate the expectation values when these operators come into play, we have to introduce some tricks. Let us first consider the eigenvalues and eigenstates of the Pauli-$X$ matrix. They are given by
\begin{align}
    \label{eq:PauliXEigenvaluesAndEigenstates}
    X\frac{1}{\sqrt{2}}(\ket{0} + \ket{1}) = \frac{1}{\sqrt{2}}(\ket{0} + \ket{1}) \notag \\
    X\frac{1}{\sqrt{2}}(\ket{0} - \ket{1}) = - \frac{1}{\sqrt{2}}(\ket{0} - \ket{1}),
\end{align}
where we see that the top state and bottom state has an eigenvalue of 1 and -1 respectively. Let us see what happens if we apply a Hadamard gate (eq. (\ref{eq:HadamardGate})) to the first eigenstate:
$$H\frac{1}{\sqrt{2}}(\ket{0} + \ket{1}) = \ket{0}. $$
Likewise with the second eigenstate:
$$H\frac{1}{\sqrt{2}}(\ket{0} - \ket{1}) = \ket{1}. $$
Hence, the same gate applied to each of these states transforms them into their own computational basis state. This means that we can obtain the corresponding eigenvalue by running this circuit
\begin{equation}
   \label{circuit:ExpectationValuePauliX}
    \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{X} & \gate{H} & \meter 
}
\end{array},
\end{equation}
and conclude that we have obtained an eigenvalue of 1 when we measure the qubit in the $\ket{0}$ state and an eigenvalue of -1 when we measure the qubit in the $\ket{1}$ state. For the Pauli-$Y$ matrix, we have the following eigenvalues and eigenstates
\begin{align}
    \label{eq:PauliYEigenvaluesAndEigenstates}
    Y\frac{1}{2}(\ket{0} + i\ket{1}) = \frac{1}{2}(\ket{0} + i\ket{1}) \notag \\
    Y\frac{1}{2}(\ket{0} - i\ket{1}) = -\frac{1}{2}(\ket{0} - i\ket{1}),
\end{align}
where the first eigenstate and the second eigenstate has an eigenvalue of 1 and -1, respectively. 
As we did with the Pauli-$X$ matrix, we can revert each of the eigenstates back to the computational basis with a unitary transformation. Consider
$$HS\frac{1}{2}(\ket{0} + i\ket{1}) = H\frac{1}{\sqrt{2}}(\ket{0} + \ket{1}) = \ket{0},$$
and
$$ HS\frac{1}{2}(\ket{0} - i\ket{1}) = H\frac{1}{\sqrt{2}}(\ket{0} - \ket{1}) = \ket{1},$$
where $S$ is the phase shift gate, see eq. (\ref{eq:Sgate}).
Hence, we can find the eigenvalues with this circuit
\begin{equation}
   \label{circuit:ExpectationValuePauliX}
    \begin{array}{c}
\Qcircuit @C=2em @R=1em {
& \gate{Y} &\gate{S} &  \gate{H} & \meter 
}
\end{array},
\end{equation}
and follow the same conclusions as we did for the Pauli-$X$ gate.

\subsection{Variational ansatz / Trial state}
\label{subsec:VariaAnsatz}

Before running the circuits to obtain the expectation values, we need to have set up a parametrized variational state. The variational state $\ket{\psi_T(\boldsymbol{\theta})}$, also called the wavefunction ansatz, is usually set up with a combination of some unitary transformation which causes sufficient entanglement $U_{ent}$ between the qubits and some sort of rotation $U_a(\boldsymbol{\theta})$ on the qubits depending on the angles $\boldsymbol{\theta} = [\theta_1, \theta_2, \cdots , \theta_p]$. We will consider a couple of ansatzes in this thesis. The simplest one consists of only $R_y(\theta)$ gates (eq. (\ref{eq:RotationOps})) and CNOT gates (eq. (\ref{eq:CNOTMatrix})). The circuit to initialize this ansatz is shown below
\begin{equation}
    \label{circuit:VQERyAnsatz}
    \ket{\psi_T (\boldsymbol{\theta})} = \begin{array}{c}
\Qcircuit @C=2em @R=1em {
 & \gate{R_y(\theta_1)} & \ctrl{1}      & \qw      & \qw     &   \cdots \\
& \gate{R_y(\theta_2)}     & \targ & \ctrl{1}      & \qw     & \cdots  \\
& \gate{R_y(\theta_3)}     & \qw      & \targ & \ctrl{1}     & \cdots  \\
& \gate{R_y(\theta_4)}& \qw    & \qw   & \targ    & \cdots \\
&  &  & \vdots &  & & \\
}
\end{array},
\end{equation}

\bigskip

where the same pattern of applying $R_y(\theta)$- gates and CNOT gates to neighbooring qubits continues til we reach the final qubit.\newline
Since an arbitrary single-qubit Euler rotation can be expressed in terms of a combination of $R_z$ and $R_x$ gates, the second ansatz will be prepared with some entanglement gate $U_{ent}$ interleaved between such arbitrary rotations. This type of trial state for $n$ qubits can be written as \cite{MaxCutAndEulerRotationHardwareEfficient}
\begin{align}
    \label{eq:EulerRotationTrialState}
    \ket{\psi_T(\boldsymbol{\theta}} &= \left[\prod_{q=1}^n U^{[q,d]}(\boldsymbol{\theta}^{[q,d]} ) \right] \times U_{ent} \times \left[\prod_{q=1}^n U^{[q,d-1]}(\boldsymbol{\theta}^{[q,d-1]}) \right] \times U_{ent} \notag \\
    & \times \cdots \times U_{ent} \times \left[\prod_{q=1}^n U^{[q,1]}(\boldsymbol{\theta}^{[q,1]}) \right] \ket{000\cdots 0},
\end{align}
where $n$ is the number of qubits, $d$ is the number of successive applications of $U_{ent}\left[\prod_{q=1}^n U^{[q,k]}(\boldsymbol{\theta}^{[q,k]}) \right]$, and
\begin{equation}
\label{eq:EulerRotation}
    U^{[k,l]}(\boldsymbol{\theta}^{[k,l]} ) = R_z(\theta^{[k,l]}_1) R_x(\theta^{[k,l]}_2)R_z(\theta^{[k,l]}_3).
\end{equation}
We restrict ourselves to the following entanglement gate in this thesis:
\begin{equation}
    \label{circuit:Uentvqe}
    U_{ent} \equiv \begin{array}{c}
\Qcircuit @C=1em @R=1em {
& \ctrl{1} & \qw & \qw & \qw & \qw & \qw& \qw& \qw\\
& \targ & \ctrl{1} & \qw  &\qw & \qw& \qw& \qw& \qw \\
& \qw & \targ{1} & \qw \qw[1] & \control & \qw& \qw& \qw & \qw \\
& \vdots& \vdots & \vdots & \vdots & \vdots & \vdots & \vdots  \\
& \qw & \qw & \qw & \qw & \qw & \targ & \ctrl{1}& \qw \\
& \qw& \qw& \qw& \qw& \qw & \qw & \targ  & \qw 
}
\end{array}
\end{equation}
We can see that the number of parameters required to optimize over is $3nd$.
The circuit for the Euler rotation ansatz is shown below
\begin{equation}
    \label{circuit:VQEEulerRotationAnsatz}
    \ket{\psi_T (\boldsymbol{\theta})} = \begin{array}{c}
\Qcircuit @C=1em @R=1em {
& \gate{U^{[1,1]}(\boldsymbol{\theta}^{[1,1]})} & \multigate{3}{U_{ent}} & \gate{U^{[1,2]}(\boldsymbol{\theta}^{[1,2]})} & \multigate{3}{U_{ent}} & \cdots & & \gate{U^{[1,d]}(\boldsymbol{\theta}^{[1,d]})} \\
& \gate{U^{[2,1]}(\boldsymbol{\theta}^{[2,1]})} & \ghost{{U_{ent}}} & \gate{U^{[2,2]}(\boldsymbol{\theta}^{[2,2]})} & \ghost{{U_{ent}}} & \cdots & & \gate{U^{[2,d]}(\boldsymbol{\theta}^{[2,d]})}\\
& \vdots & \nghost{U_{ent}} & \vdots & \nghost{U_{ent}} & \cdots &  \\
& \gate{U^{[n,1]}(\boldsymbol{\theta}^{[n,1]})} & \ghost{{U_{ent}}} & \gate{U^{[n,2]}(\boldsymbol{\theta}^{[n,2]})} & \ghost{{U_{ent}}} & \cdots & &\gate{U^{[n,d]}(\boldsymbol{\theta}^{[n,d]})}
}
\end{array}
\end{equation}


Even though these ansatzes are fit to solve the Max-Cut graph, there is a problem with using these for the pairing Hamiltonian: The ansatzes do not preserve the particle number. For any number of spin-orbitals, we will possibly end up in the lowest energy configuration the ansatz is flexible enough to produce. However, the ansatz is indifferent whether this is a one-particle or fifty-particle state. For the pairing Hamiltonian (eq. (\ref{eq:JordanWignerHamiltonian})), it is cruicial that we can specify the specific configuration we wish to solve for. Hence, we will now introduce a particle-number conserving ansatz, the Unitary Coupled Cluster Ansatz.

\subsection{Unitary Coupled Cluster ansatz}
\label{subsec:UCCAnsatz}

As we learned in section \ref{sec:CCD} the coupled cluster ansatz is given by 
$$\ket{\psi_{CC}}  = e^{\hat{T}} \ket{c} ,$$
with the cluster operator $\hat{T}$ given by eq. (\ref{eq:ClusterOperator}) and $\ket{c}$ being our reference state (see eq. (\ref{eq:ReferenceState})). This type of ansatz is not implementable on a quantum computer since $e^{\hat{T}}$ is not a unitary operator. Unitary coupled cluster instead suggest that we write our ansatz as
\begin{equation}
    \label{eq:UnitaryCoupledClusterAnsatz}
    \ket{\psi_{UCC}} = e^{\hat{T} - \hat{T}^\dagger}\ket{\psi_0},
\end{equation}
where $\hat{T}$ is the usual coupled cluster operator in eq. (\ref{eq:ClusterOperator}) and the state $\ket{\psi_0}$ is the reference state in eq. (\ref{eq:qubitReferenceState}).
One can show that $e^{\hat{T} - \hat{T}^\dagger}$ is unitary \cite{UCCDArticle}.
For this thesis, we will restrict ourselves to the unitary coupled cluster doubles (UCCD) method and hence we will deal with the cluster operator
\begin{align}
    \label{eq:UnitaryOp}
    \hat{T}_{UCCD} = \hat{T} - \hat{T}^\dagger &=  \notag \\
     &\sum_{ijab} t_{ij}^{ab}(a^\dagger_a a^\dagger_b a_j a_i - a^\dagger_i a^\dagger_j a_b a_a),
\end{align}
where we vary the trial wavefunction over the real cluster amplitudes $t_{ij}^{ab}$. Note that our ansatz is dependent on the number of particles in our system as we sum over $i$ and $j$. Hence, we can specify the specific configuration we wish to solve for, unlike the previously discussed ansatzes. 
Rewriting eq. (\ref{eq:UnitaryOp}) in terms of Pauli gates by utilizing the Jordan-Wigner transformation (see section \ref{subsec:JordanWignerTransformation}) gives \cite{UCCDArticle}
\begin{align}
    \label{eq:UCCDoublesGates}
    t_{ij}^{ab}(a^\dagger_a a^\dagger_b a_j a_i - a^\dagger_i a^\dagger_j a_b a_a) &= \frac{it_{ij}^{ab}}{8} \bigotimes_{k=i+1}^{j-1}\sigma_z^k \bigotimes_{l=a+1}^{b-1}\sigma_z^l \notag \\
    &(\sigma_x^i \sigma_x^j \sigma_y^a \sigma_x^b + \sigma_y^i \sigma_x^j \sigma_y^a \sigma_y^b \notag \\
    +&\sigma_x^i\sigma_y^j \sigma_y^a \sigma_y^b + \sigma_x^i \sigma_x^j \sigma_x^a \sigma_y^b \notag \\
    -& \sigma_y^i \sigma_x^j \sigma_x^a \sigma_x^b - \sigma_x^i \sigma_y^j \sigma_x^a \sigma_x^b \notag \\
    -&\sigma_y^i \sigma_y^j \sigma_y^a \sigma_x^b - \sigma_y^i \sigma_y^j \sigma_x^a \sigma_y^b ),
\end{align}
where we can assume that $i < j < a < b$. The subscript denotes which qubit we act upon with the Pauli gates.
The preparation of this trial wavefunction is done on a quantum computing by first utilizing the Suzuki-Trotter approximation (see section \ref{subsec:SuzukiTrotter}) on the operator in eq. (\ref{eq:UCCDoublesGates}). Denoting 
$$\hat{Z}_{ij}^{ab} =i\frac{t_{ij}^{ab}}{8p}(\bigotimes_{k=i+1}^{j-1}\sigma_z^k)(\bigotimes_{l=a+1}^{b-1}\sigma_l^z),$$ 
the Suzuki-Trotter approximation gives
\begin{align}
    \label{eq:UCCTrotterApprox}
    \ket{\psi(\boldsymbol{t})} &\approx  \bigg( \prod_{ijab} e^{\hat{Z}_{ij}^{ab}\sigma_x^i \sigma_x^j \sigma_y^a \sigma_x^b }
    e^{\hat{Z}_{ij}^{ab}\sigma_y^i \sigma_x^j \sigma_y^a \sigma_y^b }
    e^{\hat{Z}_{ij}^{ab}\sigma_x^i\sigma_y^j \sigma_y^a \sigma_y^b }
    e^{\hat{Z}_{ij}^{ab}\sigma_x^i \sigma_x^j \sigma_x^a \sigma_y^b } \notag \\
    &e^{-\hat{Z}_{ij}^{ab}\sigma_y^i \sigma_x^j \sigma_x^a \sigma_x^b }
    e^{-\hat{Z}_{ij}^{ab}\sigma_x^i \sigma_y^j \sigma_x^a \sigma_x^b }
    e^{-\hat{Z}_{ij}^{ab}\sigma_y^i \sigma_y^j \sigma_y^a \sigma_x^b }
    e^{-\hat{Z}_{ij}^{ab}\sigma_y^i \sigma_y^j \sigma_x^a \sigma_y^b }
    \bigg)^p \ket{c},
\end{align}
where $p$ decides the step size in the Suzuki-Trotter approximation and will be restricted to $p=1$ in this thesis. The operator in eq. (\ref{eq:UCCTrotterApprox}) can be implemented with Hamiltonian simulation (see section \ref{subsec:HamiltonianSimulation}) by utilizing circuit (\ref{circuit:TimeEvolutionArbitraryPauli}).
Since we are dealing with the pairing Hamiltonian (eq. (\ref{eq:SimplifiedPairingHamiltonian})) in this thesis, we can reduce the number of terms in our ansatz by not allowing to break particle pairs. The Taylor expansion of the UCCD operator gives us
\begin{align}
    e^{\sum_{ijab} t_{ij}^{ab}(a^\dagger_a a^\dagger_b a_j a_i - a^\dagger_i a^\dagger_j a_b a_a)} &= I + \sum_{ijab} t_{ij}^{ab}(a^\dagger_a a^\dagger_b a_j a_i - a^\dagger_i a^\dagger_j a_b a_a) \notag \\
    & + [\sum_{ijab} t_{ij}^{ab}(a^\dagger_a a^\dagger_b a_j a_i - a^\dagger_i a^\dagger_j a_b a_a)]^2/2! \notag \\
    &+ \cdots.
\end{align}
We can immediately see that the first sum over $i,j,a,b$ will break pairs if we do not introduce a restriction on the sum. This can be explained by the fact that any non zero $t_{ij}^{ab}$ will include the following term
$$
t_{ij}^{ab}(a^\dagger_a a^\dagger_b a_j a_i - a^\dagger_i a^\dagger_j a_b a_a)\ket{c},
$$
which will break pairs if $j \neq i+1$ and $b \neq a+1$. 
Hence we make the restriction $j = i+1$ and $b = a+1$.

\subsection{Simple ansatz for one pair and four spin-orbitals}
\label{subsec:SimplePairingAnsatz}
For one pair and four spin-orbitals, one can set up a simple ansatz for the pairing model (eq. (\ref{eq:SimplifiedPairingHamiltonian})) with the following circuit
\begin{equation}
    \label{circuit:SimplePairingCircuit}
    \ket{\psi_T (\boldsymbol{\theta})} = \begin{array}{c}
\Qcircuit @C=1em @R=1em {
& \gate{R_y(\theta)} &\ctrl{1} & \qw& \qw& \qw& \qw \\
& \qw & \targ & \gate{X} & \ctrl{1} & \ctrl{2} & \gate{X}  \\
& \qw & \qw & \qw & \targ & \qw & \qw \\
& \qw& \qw& \qw& \qw & \targ & \qw \\
}
\end{array},
\end{equation}
where all qubits are initialized in the $\ket{0}$ state.
Too see that this ansatz conserves the particle number, we can write it out mathematically.
First is the application of the $R_y(\theta)$ gate (see eq. (\ref{eq:RotationOps})). This gives us the state
$$
(\cos{\frac{\theta}{2}}\ket{0} - i\sin{\frac{\theta}{2}}\ket{1}) \ket{000}.
$$
The CNOT (see eq. (\ref{eq:CNOTMatrix})) entangles the second qubit with the first, giving us
$$\cos{\frac{\theta}{2}}\ket{0000} - i\sin{\frac{\theta}{2}}\ket{1100}. $$
The subsequent gates flip both of the bottom qubits if the second qubit is in the $\ket{0}$ state. This results in the state
$$\cos{\frac{\theta}{2}}\ket{0011} - i\sin{\frac{\theta}{2}}\ket{1100}, $$
which we can see is a parametrized linear combination of the two allowed states.

\subsection{Summary of the variational quantum eigensolver algorithm}
We can summarize the variational quantum eigensolver algorithm as follows
\begin{itemize}
    \item Apply a parametrized unitary operator (ansatz) $U(\boldsymbol{\theta})$ to an $n$-qubit state. This yields the state $\ket{\psi(\boldsymbol{\theta})} = U(\boldsymbol{\theta})\ket{\psi}$. Examples of ansatzes are given in sections \ref{subsec:UCCAnsatz}, \ref{subsec:VariaAnsatz} and \ref{subsec:SimplePairingAnsatz}.
    \item Calculate the energy expectation value (eq. (\ref{eq:VQEExpectationValueofHamiltonian})) for a Jordan-Wigner transformed (section \ref{subsec:JordanWignerTransformation}) Hamiltonian. This is done by evaluating the expectation value of each separate term of the Hamiltonian, as described in section \ref{subsec:VQEExpecVals}.
    \item Vary the parameters $\boldsymbol{\theta}$ and do the same procedure till the energy expectation value is minimized.
\end{itemize}

\section{Quantum Adiabatic Time Evolution}
\label{sec:QATE}

The adiabatic theorem states that if we start out in the ground state $\Psi_0$ of a Hamiltonian $\hat{H}_0$ and gradually change the Hamiltonian of the system to $\hat{H}_1$, we will eventually end up in the ground state $\Psi_1$ of $\hat{H}_1$ given that the gradual change is small enough \cite{AdiabaticTimeEvolution}. This provides the concept for adiabatic quantum computing, an alternative to the standard circuit model of quantum computing. Here we will, however, see how this theorem could be implemented on the standard circuit model to find the ground state energy of a fermionic Hamiltonian. We call this method the quantum adiabatic time evolution (QATE) algorithm. We start out with a mathematical formulation of the change from $\hat{H}_0$ to $\hat{H}_1$. This can be written as a new time dependent Hamiltonian
\begin{equation}
    \label{eq:AdiabaticHamiltonian}
    \hat{H}(t) = (1-\frac{t}{T})\hat{H}_0 + \frac{t}{T}\hat{H}_1,
\end{equation}
with $t \in [0,T]$. Since we are now dealing with a time-dependent Hamiltonian, the time evolution operator is now given by the time-ordered exponential \cite{TimeOrderedExponential}
\begin{equation}
    \label{eq:QATETimeOrderedExponential}
    U(t) = e^{-i\int_0^t H(t) dt }.
\end{equation}
We can approximate the integral by utilizing numerical integration. Breaking the time interval into $n$ time steps and utilizing the midpoint rule \cite{midpointrule} gives
\begin{align}
    \label{eq:RectangularIntegration}
    \int_0^t H(t) dt &\approx \sum_{k=0}^{n}H(k\Delta t)\Delta t \notag \\
    &= \sum_{k=0}^{n} \left[(1-\frac{k\Delta t}{T})\hat{H}_0 + \frac{k\Delta t}{T}\hat{H}_1 \right] \Delta t,
\end{align}
with $\Delta t = \frac{T}{n}$.
The approximation to the time-ordered exponential operator is then given by
\begin{equation}
    \label{eq:TimeOrderedExponentialNumericalIntegration}
    U(t) = e^{-i\sum_{k=0}^{n} \left[(1-\frac{k\Delta t}{T})\hat{H}_0 + \frac{k\Delta t}{T}\hat{H}_1 \right] \Delta t}.
\end{equation}
Provided we utilize small time steps, we can use the Suzuki-Trotter approximation (section \ref{subsec:SuzukiTrotter}) to yield the following operator
\begin{equation}
    \label{eq:QATETrotterApproximation}
    U(t) \approx \prod_{k=0}^{n}e^{-i\left[(1-\frac{k\Delta t}{T})\hat{H}_0 + \frac{k\Delta t}{T}\hat{H}_1 \right] \Delta t},
\end{equation}
which is effectively applied with the time evolution circuit (\ref{circuit:TimeEvolutionArbitraryPauli}).

When later using QATE to solve for the ground state energy of the pairing Hamiltonian (eq. (\ref{eq:JordanWignerHamiltonian})), we will put the initial Hamiltonian to
\begin{equation}
    \label{eq:QATEInitialHamiltonian}
    \hat{H}_0 = \frac{1}{5}[\sum_{i=1}^{n_f} \sigma_z - \sum_{i=n_f+1}^{n_s}\sigma_z],
\end{equation}
where $n_f$ is the number of particles we wish to solve for and $n_s$ is the number of spin-orbitals. This Hamiltonian is chosen because of its simplicity, only adding terms linear in the amount of qubits. It also conserves the particle number. The ground state of this Hamiltonian, which also will be our initial state for the QATE algorithm is
\begin{equation}
    \label{eq:QATEInitialState}
    \ket{\psi_0} = \ket{111\cdots\ 1 000\cdots 0},
\end{equation}
that is, the $n_f$ last qubits are put to zero, while the rest are put to one.

\section{Validating the results}
When dealing with the VQE and the QATE algorithm, a natural question to ask is how we know that we have reached an eigenstate $\ket{\psi_k}$ of our Hamiltonian $\hat{H}$. When measuring the energy expectation value of our system, we know from quantum mechanics that the variance of the energy estimate should be zero when the measured state is an eigenstate of our Hamiltonian. The variance can evaluated the following way
\begin{equation}
    \label{eq:EnergyVariance}
    \sigma^2_E = \bra{\psi}\hat{H}^2\ket{\psi} - \bra{\psi} \hat{H} \ket{\psi}^2.
\end{equation}
For the VQE algorhtm, we could minimize the energy and finally evaluate eq. (\ref{eq:EnergyVariance}) to make sure it is below some threshold. For the QATE algorithm, we could measure the variance at each time step and stop the algorithm if the variance is below some threshold. Even though the variance tells us if we have reached an eigenstate or not, we can not be certain that we have not reached one of the excited states rather than the ground state.

